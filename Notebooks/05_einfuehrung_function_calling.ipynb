{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbbdc2c",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Einf√ºhrung: Function Calling mit vLLM\n",
    "\n",
    "In dem Notebook lernen wir, wie man LLMs dazu bringt, Funktionen aufzurufen und Tool-Interaktionen durchzuf√ºhren.  \n",
    "\n",
    "## üìö Quellen\n",
    "\n",
    "- [vLLM: Tool Calling Documentation](https://docs.vllm.ai/en/latest/features/tool_calling.html)\n",
    "- [OpenAI: Function Calling API](https://platform.openai.com/docs/guides/function-calling)\n",
    "\n",
    "---\n",
    "\n",
    "Viel Erfolg beim Ausprobieren der Function Calling Features! ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19b36f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f601f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_URL = \"http://132.199.138.16:11434\"\n",
    "LLM_MODEL = \"gpt-oss:20b\" # Nicht jedes LLM unterst√ºtzt Function Calling. Gemma3 beispielsweise wurde nicht daf√ºr trainiert. Das OpenSource LLM \"gpt-oss:20b\" von OpenAI hingegen schon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a0239e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# Verwenden wir zur Abwechslung mal die ollama Bibliothek statt openai. Tool Calling wird auch von openai unterst√ºtzt.\n",
    "# Siehe hier: https://ollama.com/blog/tool-support\n",
    "client = Client(\n",
    "  host=LLM_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b3580",
   "metadata": {},
   "source": [
    "### 1. Beispiel: Tool Call mit Aktienkursen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c2efe",
   "metadata": {},
   "source": [
    "Definieren wir zun√§chst eine Liste mit Tools. In unserem Beispiel nur **ein** Tool, das den aktuellen Aktienkurs f√ºr ein gegebenes Symbol zur√ºckgibt. \n",
    "Die `openai`-Bibliothek erwartet jedes Tool in einem bestimmten JSON-Format. \n",
    "\n",
    "* `type`: OpenAI bietet prinzipiell auch andere Tool-Typen an (z.B. `web_search`), wir verwenden hier aber nur `function`.\n",
    "* `name`: Der Name des Tools.\n",
    "* `parameters`: Die Parameter, die die Funktion erwartet.\n",
    "* `required`: Welche Parameter zwingend √ºbergeben werden m√ºssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fdc5e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "            'name': 'get_stock_price',\n",
    "            'description': 'Get the current stock price for a company',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'symbol': {\n",
    "                        'type': 'string',\n",
    "                    },\n",
    "                },\n",
    "                'required': ['symbol'],\n",
    "            },\n",
    "    },\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e04e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [ToolCall(function=Function(name='get_stock_price', arguments={'symbol': 'AAPL'}))]\n",
      "Function name: get_stock_price\n",
      "Function arguments: {'symbol': 'AAPL'}\n"
     ]
    }
   ],
   "source": [
    "# Starten wir mit einem Beispiel, in dem wir den aktuellen Aktienkurs von Apple abfragen.\n",
    "response = client.chat(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{'role': 'user', 'content': 'Wie ist die aktuelle Apple Aktie?'}],\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "tool_calls = response['message']['tool_calls']\n",
    "print(f\"Tool calls: {tool_calls}\")\n",
    "# Argumente des Tool Calls und Funktion ausgeben\n",
    "print(f\"Function name: {tool_calls[0].function.name}\")\n",
    "print(f\"Function arguments: {tool_calls[0].function.arguments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34d5b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='1\\u202f+\\u202f1\\u202f=\\u202f2.' thinking='User asks in German: \"Was ist 1+1?\" meaning \"What is 1+1?\" The answer: 2.' images=None tool_name=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "# Fragen wir eine irrelevante Frage, die nichts mit Aktien zu tun hat.\n",
    "# In dem Fall ist tool_calls None, da das LLM kein Tool aufrufen muss.\n",
    "response = client.chat(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{'role': 'user', 'content': 'Was ist 1+1?'}],\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Bei der Ausgabe sehen wir, dass tool_calls None ist.\n",
    "print(response['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b145300",
   "metadata": {},
   "source": [
    "## √úbungsaufgabe: Tool Execution - Funktionen tats√§chlich ausf√ºhren\n",
    "\n",
    "Unser Wetter Tool k√∂nnen wir nun tats√§chlich implementieren mit einer echten API (siehe Beispiel-Aufruf unten). Diese stellt Wetterdaten bereit f√ºr heute und morgen.\n",
    "\n",
    "Gehen Sie f√ºr die Implementierung wie folgt vor:\n",
    "\n",
    "1. Definieren Sie eine Funktion `get_weather` im JSON Schema\n",
    "2. Implementieren Sie die Funktion `get_weather_from_api`, die die Wetter API aufruft und die Wetterdaten f√ºr einen bestimmten Ort abruft. Die Funktion gibt alle Wetter-Daten als Python Dictionary zur√ºck.\n",
    "3. Rufen Sie das LLM auf mit `get_weather` als verf√ºgbares Tool\n",
    "4. Analysieren Sie die Antwort des LLM und extrahieren Sie die Parameter f√ºr den Funktionsaufruf\n",
    "5. Rufen Sie die Funktion `get_weather_from_api` mit den extrahierten Parametern auf\n",
    "6. Geben Sie die Ergebnisse der Funktion zur√ºck an das LLM, um eine abschlie√üende Antwort zu generieren\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d329b",
   "metadata": {},
   "source": [
    "So funktioniert die API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64f62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ort: Berlin\n",
      "Aktuelle Temperatur: 12 ¬∞C\n",
      "Wetter: Light drizzle\n",
      "['0', '300', '600', '900', '1200', '1500', '1800', '2100']\n",
      "['13', '13', '11', '12', '15', '14', '13', '13']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "city = \"Berlin\"\n",
    "url = f\"http://wttr.in/{city}?format=j1\"  # JSON-Format\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "print(\"Ort:\", city)\n",
    "print(\"Aktuelle Temperatur:\", data[\"current_condition\"][0][\"temp_C\"], \"¬∞C\")\n",
    "print(\"Wetter:\", data[\"current_condition\"][0][\"weatherDesc\"][0][\"value\"])\n",
    "\n",
    "# Zugriff auf Wetterdaten\n",
    "# data[\"weather\"][0] # Wetter heute\n",
    "# data[\"weather\"][1] # Wetter morgen\n",
    "print([tempData[\"time\"] for tempData in data[\"weather\"][0][\"hourly\"]]) # Output: ['0', '300', '600', '900', '1200', '1500', '1800', '2100'], was die Stunden des Tages repr√§sentiert\n",
    "print([tempData[\"tempC\"] for tempData in data[\"weather\"][0][\"hourly\"]]) # Temperaturen heute f√ºr 0 bis 21 Uhr in 3-Stunden-Schritten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7194b74",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>L√∂sung anzeigen</b></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 1. JSON Schema Definition f√ºr get_weather Tool\n",
    "tools = [\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Ruft aktuelle Wetterdaten f√ºr eine Stadt ab',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'city': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'Name der Stadt'\n",
    "                    }\n",
    "                },\n",
    "                'required': ['city']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2. API-Funktion implementieren\n",
    "\n",
    "\n",
    "def get_weather_from_api(city):\n",
    "    \"\"\"Ruft Wetterdaten von wttr.in API ab\"\"\"\n",
    "    url = f\"http://wttr.in/{city}?format=j1\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "\n",
    "        # Strukturierte R√ºckgabe\n",
    "        weather_info = {\n",
    "            \"city\": city,\n",
    "            \"current_temp\": data[\"current_condition\"][0][\"temp_C\"],\n",
    "            \"current_weather\": data[\"current_condition\"][0][\"weatherDesc\"][0][\"value\"],\n",
    "            \"today_temps\": [temp[\"tempC\"] for temp in data[\"weather\"][0][\"hourly\"]],\n",
    "            \"today_times\": [temp[\"time\"] for temp in data[\"weather\"][0][\"hourly\"]],\n",
    "            \"tomorrow_temps\": [temp[\"tempC\"] for temp in data[\"weather\"][1][\"hourly\"]],\n",
    "            \"tomorrow_times\": [temp[\"time\"] for temp in data[\"weather\"][1][\"hourly\"]],\n",
    "            \"tomorrow_temp\": data[\"weather\"][1][\"avgtempC\"],\n",
    "        }\n",
    "        return weather_info\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Fehler beim Abrufen der Wetterdaten: {str(e)}\"}\n",
    "\n",
    "\n",
    "# 3. Vollst√§ndiger Workflow\n",
    "def weather_tool_workflow(user_question):\n",
    "\n",
    "    # F√ºhren wir zun√§chst den Tool-Call aus.\n",
    "    messages_tool_call = [{'role': 'user', 'content': user_question}]\n",
    "    response = client.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages_tool_call,\n",
    "        tools=tools,\n",
    "    )\n",
    "    tool_calls = response['message']['tool_calls']\n",
    "\n",
    "    # Pr√ºfe, ob das LLM ein Tool aufrufen w√ºrde\n",
    "    if tool_calls:\n",
    "        \n",
    "        # Speichern wir function_name und arguments in eine Variable\n",
    "        tool_call = tool_calls[0]\n",
    "        function_name = tool_call['function']['name']\n",
    "        arguments = tool_call['function']['arguments']\n",
    "\n",
    "        print(f\"Tool aufgerufen: {function_name}\")\n",
    "        print(f\"Parameter: {arguments}\")\n",
    "\n",
    "        # Rufen wir nun die \"echte\" get_wheather_from_api auf\n",
    "        if function_name == \"get_weather\":\n",
    "            weather_data = get_weather_from_api(arguments[\"city\"])\n",
    "            print(\"Abgerufene Wetterdaten:\", weather_data)\n",
    "\n",
    "        # Die urspr√ºngliche User Prompt sowie das Ergebnis vom Tool-Aufruf geben wir nun an das LLM um die\n",
    "        # Finale Antwort zu generieren. \n",
    "        # json.dumps wandelt das Ergebnis der Wetter API von einem Python Dictionary um in ein String. Das LLM unterst√ºtzt als Input immer nur Text.\n",
    "        messages = [{'role': 'user', 'content': f\"Der User hat folgende frage gestellt {user_question}. Bitte antworte in einem kleinen Flie√ütext auf die Frage.\"}]\n",
    "        messages.append({'role': 'tool', 'content': json.dumps(weather_data)})\n",
    "\n",
    "        # Finale Antwort vom LLM\n",
    "        final_response = client.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "\n",
    "        return final_response['message']['content']\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "result = weather_tool_workflow(\n",
    "    \"Wie ist das Wetter morgen am Abend in M√ºnchen?\")\n",
    "print(\"Finale Antwort:\", result)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nils_hellwig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
