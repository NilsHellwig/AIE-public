{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbbdc2c",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Notebook: Einf√ºhrung in Function Calling\n",
    "\n",
    "In dem Notebook lernen wir, wie man LLMs dazu bringt, Funktionen aufzurufen.\n",
    "\n",
    "## üìö Quellen\n",
    "\n",
    "- [OpenAI: Function Calling API](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [Ollama: Function Calling](https://ollama.com/blog/functions-as-tools)\n",
    "\n",
    "---\n",
    "\n",
    "Viel Erfolg beim Ausprobieren der Function Calling Features! ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc63a73",
   "metadata": {},
   "source": [
    "Beginnen wir damit das Python Packet von ollama zu installieren. Anders als bei den vorherigen Notebooks, wo wir die OpenAI API genutzt haben, verwenden wir in diesem Notebook zur Abwechslung mal die Ollama API.\n",
    "\n",
    "\n",
    "So wie `ollama`, bietet auch die Bibliothek `openai` die M√∂glichkeit, Requests an einen Server mit einer LLM-API zu senden. Beachte: `openai` unterst√ºtzt ebenso wie `ollama` Function Calling.\n",
    "\n",
    "[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b36f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv add ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f601f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_URL = \"http://132.199.138.16:11434\"\n",
    "# Nicht jedes LLM unterst√ºtzt Function Calling. \n",
    "# Gemma3, das LLM aus der letzten √úbung wurde beispielsweise nicht daf√ºr trainiert. Das open-source LLM \"gpt-oss:20b\" von OpenAI hingegen schon.\n",
    "LLM_MODEL = \"gpt-oss:20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0239e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(\n",
    "  host=LLM_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b3580",
   "metadata": {},
   "source": [
    "### 1. Beispiel: Tool Call mit Aktienkursen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c2efe",
   "metadata": {},
   "source": [
    "Definieren wir zun√§chst eine Liste mit Tools. In unserem Beispiel nur **ein** Tool, das den aktuellen Aktienkurs f√ºr ein gegebenes Symbol zur√ºckgibt. \n",
    "\n",
    "Die `openai`-Bibliothek erwartet jedes Tool in einem bestimmten Format. Wir definieren das Tool als Dictionary mit folgenden Schl√ºsseln:\n",
    "\n",
    "* `type`: OpenAI bietet prinzipiell auch andere Tool-Typen an (z.B. `web_search`), wir verwenden hier aber nur `function`.\n",
    "* `name`: Der Name des Tools.\n",
    "* `parameters`: Die Parameter, die die Funktion erwartet.\n",
    "* `required`: Welche Parameter zwingend √ºbergeben werden m√ºssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc5e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "            'name': 'get_stock_price', # Name des Tools\n",
    "            'description': 'Get the current stock price for a company', # Beschreibung des Tools\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'symbol': { # Die Funktion erwartet genau einen Parameter, \"symbol\". Symbol ist das B√∂rsenk√ºrzel der Aktie, z.B. \"AAPL\" f√ºr Apple.\n",
    "                        'type': 'string', # Typ des Parameters\n",
    "                    },\n",
    "                },\n",
    "                'required': ['symbol'], # Erforderliche Parameter\n",
    "            },\n",
    "    },\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e04e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [ToolCall(function=Function(name='get_stock_price', arguments={'symbol': 'SAP'}))]\n",
      "Function name: get_stock_price\n",
      "Function arguments: {'symbol': 'SAP'}\n"
     ]
    }
   ],
   "source": [
    "# Starten wir mit einem Beispiel, in dem wir den aktuellen Aktienkurs von SAP abfragen.\n",
    "response = client.chat(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{'role': 'user', 'content': 'Wie steht die aktuelle SAP Aktie?'}], # Benutzeranfrage\n",
    "    tools=tools, # Wir √ºbergeben die Tools, die das Modell verwenden kann\n",
    ")\n",
    "\n",
    "# Wir k√∂nnen uns nun anschauen, ob und wie das Modell das Tool aufgerufen hat.\n",
    "tool_calls = response['message']['tool_calls']\n",
    "print(f\"Tool calls: {tool_calls}\")\n",
    "\n",
    "# Argumente des Tool Calls und Funktion ausgeben\n",
    "print(f\"Function name: {tool_calls[0].function.name}\")\n",
    "print(f\"Function arguments: {tool_calls[0].function.arguments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d5b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='gpt-oss:20b' created_at='2025-11-25T19:07:41.864122Z' done=True done_reason='stop' total_duration=659379875 load_duration=143220666 prompt_eval_count=131 prompt_eval_duration=48156583 eval_count=46 eval_duration=455991580 message=Message(role='assistant', content='1\\u202f+\\u202f1\\u202f=\\u202f2.', thinking='User asks: \"Was ist 1+1?\" Means \"What is 1+1?\" The answer is 2.', images=None, tool_name=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Wir k√∂nnen probeweise mal eine irrelevante Frage stellen, die nichts mit Aktien zu tun hat.\n",
    "# In dem Fall ist tool_calls None, da das LLM kein Tool aufrufen muss.\n",
    "response = client.chat(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{'role': 'user', 'content': 'Was ist 1+1?'}],\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Schauen wir uns die Ausgabe an. Bei der Ausgabe sehen wir, dass tool_calls None ist.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b145300",
   "metadata": {},
   "source": [
    "## √úbungsaufgabe: Tool Execution - Funktionen tats√§chlich ausf√ºhren\n",
    "\n",
    "Bislang haben wir nur implementiert, dass das LLM ein Tool ggf. mit Parametern aufruft, aber die eigentliche Funktion dahinter wird nicht ausgef√ºhrt.\n",
    "\n",
    "Implementieren wir nun mal eine kleine Pipeline, die tats√§chlich eine Funktion ausf√ºhrt, wenn das LLM ein Tool aufruft. Wir implementieren jetzt ein Tool, das Informationen √ºber L√§nder abruft.\n",
    "\n",
    "Gehen Sie f√ºr die Implementierung wie folgt vor:\n",
    "\n",
    "1. Definieren Sie ein Tool `get_country_info` im JSON Schema mit Parameter `country`\n",
    "2. Implementieren Sie die Funktion `get_country_info_from_api`, die L√§nder-Infos von der API abruft\n",
    "3. Bauen Sie einen einfachen Workflow, der das LLM fragt und dann die Funktion aufruft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d329b",
   "metadata": {},
   "source": [
    "So funktioniert die API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64f62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Germany\n",
      "Hauptstadt: Berlin\n",
      "Bev√∂lkerung: 83491249\n",
      "Region: Europe\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "country = \"Germany\"  # Auch m√∂glich: France, Italy, Japan, ...\n",
    "url = f\"https://restcountries.com/v3.1/name/{country}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# API gibt eine Liste zur√ºck, wir nehmen das erste Element\n",
    "country_data = data[0]\n",
    "print(f\"Name: {country_data['name']['common']}\")\n",
    "print(f\"Hauptstadt: {country_data['capital'][0]}\")\n",
    "print(f\"Bev√∂lkerung: {country_data['population']}\")\n",
    "print(f\"Region: {country_data['region']}\")\n",
    "\n",
    "# Schreiben Sie hier Ihren Code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7194b74",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>L√∂sung anzeigen</b></summary>\n",
    "\n",
    "```python\n",
    "# 1. Tool Definition\n",
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_country_info',\n",
    "        'description': 'Ruft Informationen √ºber ein Land ab',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'country': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Der Name des Landes (z.B. Germany, France, Japan)'\n",
    "                }\n",
    "            },\n",
    "            'required': ['country']\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "# 2. API-Funktion\n",
    "def get_country_info_from_api(country):\n",
    "    url = f\"https://restcountries.com/v3.1/name/{country}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()[0]\n",
    "\n",
    "    # Wir k√∂nnen die Informationen zu einem Text zusammenfassen:\n",
    "    info = f\"Land: {data['name']['common']}, Hauptstadt: {data['capital'][0]}, Bev√∂lkerung: {data['population']}, Region: {data['region']}\"\n",
    "    return info\n",
    "\n",
    "\n",
    "# 3. Workflow\n",
    "messages = [{'role': 'user', 'content': 'Erz√§hl mir etwas √ºber Norwegen.'}]\n",
    "\n",
    "# LLM fragen\n",
    "response = client.chat(model=LLM_MODEL,\n",
    "                       messages=messages,\n",
    "                       tools=tools,\n",
    "                       options={\"temperature\": 0.0})\n",
    "\n",
    "# Wurde ein Tool aufgerufen? Pr√ºfen wir, ob der Key ['message']['tool_calls'] existiert\n",
    "if response['message']['tool_calls']:\n",
    "    country = response['message']['tool_calls'][0]['function']['arguments']['country']\n",
    "\n",
    "    # Funktion ausf√ºhren\n",
    "    country_info = get_country_info_from_api(country)\n",
    "\n",
    "    # Prompt erstellen f√ºr finale Antwort\n",
    "    prompt = f\"Hier sind die Informationen √ºber {country}:\\n{country_info}.\\nBitte fasse diese Informationen in zwei S√§tzen kurz zusammen.\"\n",
    "\n",
    "    # Zur√ºck ans LLM\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    final_response = client.chat(\n",
    "        model=LLM_MODEL, messages=messages, options={\"temperature\": 0.0})\n",
    "\n",
    "    print(f\"\\nFinale Antwort: {final_response['message']['content']}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nils_hellwig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
