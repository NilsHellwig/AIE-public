{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbbdc2c",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Notebook: Einf√ºhrung in Function Calling\n",
    "\n",
    "In dem Notebook lernen wir, wie man LLMs dazu bringt, Funktionen aufzurufen und Tool-Interaktionen durchzuf√ºhren.  \n",
    "\n",
    "## üìö Quellen\n",
    "\n",
    "- [OpenAI: Function Calling API](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [Ollama: Function Calling](https://ollama.com/blog/functions-as-tools)\n",
    "\n",
    "---\n",
    "\n",
    "Viel Erfolg beim Ausprobieren der Function Calling Features! ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc63a73",
   "metadata": {},
   "source": [
    "Beginnen wir damit ollama zu installieren. Anders als bei den vorherigen Notebooks, wo wir die OpenAI API genutzt haben, verwenden wir in diesem Notebook zur Abwechslung mal die Ollama API.\n",
    "\n",
    "\n",
    "So wie `ollama`, bietet auch die Bibliothek `openai` die M√∂glichkeit, Requests an den Server mit den LLMs zu senden und unterst√ºtzt ebenfalls Function Calling.\n",
    "\n",
    "[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b36f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv add ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f601f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_URL = \"http://132.199.138.16:11434\"\n",
    "LLM_MODEL = \"gpt-oss:20b\" # Nicht jedes LLM unterst√ºtzt Function Calling. Gemma3 beispielsweise wurde nicht daf√ºr trainiert. Das OpenSource LLM \"gpt-oss:20b\" von OpenAI hingegen schon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a0239e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# Verwenden wir zur Abwechslung mal die ollama Bibliothek statt openai. Tool Calling wird auch von openai unterst√ºtzt.\n",
    "# Siehe hier: https://ollama.com/blog/tool-support\n",
    "client = Client(\n",
    "  host=LLM_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b3580",
   "metadata": {},
   "source": [
    "### 1. Beispiel: Tool Call mit Aktienkursen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c2efe",
   "metadata": {},
   "source": [
    "Definieren wir zun√§chst eine Liste mit Tools. In unserem Beispiel nur **ein** Tool, das den aktuellen Aktienkurs f√ºr ein gegebenes Symbol zur√ºckgibt. \n",
    "Die `openai`-Bibliothek erwartet jedes Tool in einem bestimmten JSON-Format. \n",
    "\n",
    "* `type`: OpenAI bietet prinzipiell auch andere Tool-Typen an (z.B. `web_search`), wir verwenden hier aber nur `function`.\n",
    "* `name`: Der Name des Tools.\n",
    "* `parameters`: Die Parameter, die die Funktion erwartet.\n",
    "* `required`: Welche Parameter zwingend √ºbergeben werden m√ºssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc5e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "            'name': 'get_stock_price',\n",
    "            'description': 'Get the current stock price for a company',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'symbol': {\n",
    "                        'type': 'string',\n",
    "                    },\n",
    "                },\n",
    "                'required': ['symbol'],\n",
    "            },\n",
    "    },\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e04e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [ToolCall(function=Function(name='get_stock_price', arguments={'symbol': 'SAP'}))]\n",
      "Function name: get_stock_price\n",
      "Function arguments: {'symbol': 'SAP'}\n"
     ]
    }
   ],
   "source": [
    "# Starten wir mit einem Beispiel, in dem wir den aktuellen Aktienkurs von SAP abfragen.\n",
    "response = client.chat(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{'role': 'user', 'content': 'Wie steht die aktuelle SAP Aktie?'}],\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "tool_calls = response['message']['tool_calls']\n",
    "print(f\"Tool calls: {tool_calls}\")\n",
    "# Argumente des Tool Calls und Funktion ausgeben\n",
    "print(f\"Function name: {tool_calls[0].function.name}\")\n",
    "print(f\"Function arguments: {tool_calls[0].function.arguments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d5b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='gpt-oss:20b' created_at='2025-11-21T17:49:39.929289Z' done=True done_reason='stop' total_duration=750125292 load_duration=148188708 prompt_eval_count=131 prompt_eval_duration=48113292 eval_count=54 eval_duration=539943919 message=Message(role='assistant', content='1\\u202f+\\u202f1\\u202f=\\u202f2.', thinking='The user asked in German: \"Was ist 1+1?\" which translates to \"What is 1+1?\" The answer is 2. Provide answer.', images=None, tool_name=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Fragen wir eine irrelevante Frage, die nichts mit Aktien zu tun hat.\n",
    "# In dem Fall ist tool_calls None, da das LLM kein Tool aufrufen muss.\n",
    "response = client.chat(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{'role': 'user', 'content': 'Was ist 1+1?'}],\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "# Schauen wir uns die Ausgabe an. Bei der Ausgabe sehen wir, dass tool_calls None ist.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b145300",
   "metadata": {},
   "source": [
    "## √úbungsaufgabe: Tool Execution - Funktionen tats√§chlich ausf√ºhren\n",
    "\n",
    "Wir implementieren jetzt ein Tool, das Informationen √ºber L√§nder abruft. Bislang haben wir nur implementiert, dass das LLM ein Tool ggf. mit Parametern aufruft, aber die eigentliche Funktion dahinter wird nicht ausgef√ºhrt.\n",
    "\n",
    "Gehen Sie f√ºr die Implementierung wie folgt vor:\n",
    "\n",
    "1. Definieren Sie ein Tool `get_country_info` im JSON Schema mit Parameter `country`\n",
    "2. Implementieren Sie die Funktion `get_country_info_from_api`, die L√§nder-Infos von der API abruft\n",
    "3. Bauen Sie einen einfachen Workflow, der das LLM fragt und dann die Funktion aufruft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d329b",
   "metadata": {},
   "source": [
    "So funktioniert die API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a64f62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Germany\n",
      "Hauptstadt: Berlin\n",
      "Bev√∂lkerung: 83491249\n",
      "Region: Europe\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "country = \"Germany\"  # Auch m√∂glich: France, Italy, Japan, ...\n",
    "url = f\"https://restcountries.com/v3.1/name/{country}\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# API gibt eine Liste zur√ºck, wir nehmen das erste Element\n",
    "country_data = data[0]\n",
    "print(f\"Name: {country_data['name']['common']}\")\n",
    "print(f\"Hauptstadt: {country_data['capital'][0]}\")\n",
    "print(f\"Bev√∂lkerung: {country_data['population']}\")\n",
    "print(f\"Region: {country_data['region']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7194b74",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>L√∂sung anzeigen</b></summary>\n",
    "\n",
    "```python\n",
    "# 1. Tool Definition\n",
    "tools = [{\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_country_info',\n",
    "        'description': 'Ruft Informationen √ºber ein Land ab',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'country': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Der Name des Landes (z.B. Germany, France, Japan)'\n",
    "                }\n",
    "            },\n",
    "            'required': ['country']\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "# 2. API-Funktion\n",
    "def get_country_info_from_api(country):\n",
    "    url = f\"https://restcountries.com/v3.1/name/{country}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()[0]\n",
    "\n",
    "    # Wir k√∂nnen die Informationen zu einem Text zusammenfassen:\n",
    "    info = f\"Land: {data['name']['common']}, Hauptstadt: {data['capital'][0]}, Bev√∂lkerung: {data['population']}, Region: {data['region']}\"\n",
    "    return info\n",
    "\n",
    "\n",
    "# 3. Workflow\n",
    "messages = [{'role': 'user', 'content': 'Erz√§hl mir etwas √ºber Norwegen.'}]\n",
    "\n",
    "# LLM fragen\n",
    "response = client.chat(model=LLM_MODEL,\n",
    "                       messages=messages,\n",
    "                       tools=tools,\n",
    "                       options={\"temperature\": 0.0})\n",
    "\n",
    "# Wurde ein Tool aufgerufen? Pr√ºfen wir, ob der Key ['message']['tool_calls'] existiert\n",
    "if response['message']['tool_calls']:\n",
    "    country = response['message']['tool_calls'][0]['function']['arguments']['country']\n",
    "\n",
    "    # Funktion ausf√ºhren\n",
    "    country_info = get_country_info_from_api(country)\n",
    "\n",
    "    # Prompt erstellen f√ºr finale Antwort\n",
    "    prompt = f\"Hier sind die Informationen √ºber {country}:\\n{country_info}.\\nBitte fasse diese Informationen in zwei S√§tzen kurz zusammen.\"\n",
    "\n",
    "    # Zur√ºck ans LLM\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    final_response = client.chat(\n",
    "        model=LLM_MODEL, messages=messages, options={\"temperature\": 0.0})\n",
    "\n",
    "    print(f\"\\nFinale Antwort: {final_response['message']['content']}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nils_hellwig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
