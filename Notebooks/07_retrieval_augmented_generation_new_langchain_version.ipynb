{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edb5de3",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Notebook: Einf√ºhrung in Retrieval-Augmented Generation (RAG) [mit Langchain v1]\n",
    "\n",
    "In diesem Notebook lernen wir, wie wir **Retrieval-Augmented Generation (RAG)** nutzen k√∂nnen, um die Antworten von gro√üen Sprachmodellen (LLMs) durch externe Wissensquellen zu verbessern.\n",
    "\n",
    "## üìö Quellen\n",
    "\n",
    "- [LangChain Dokumentation zu RAG](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "## üîÑ √Ñnderungen\n",
    "\n",
    "Dieses Notebook wurde auf **LangChain 1.x** aktualisiert. Die Haupt√§nderungen sind:\n",
    "\n",
    "- **LangChain-Version**: Aktualisiert von 0.3.27 auf 1.1.3 (und zugeh√∂rige Pakete).\n",
    "- **Veraltete Komponenten entfernt**: `RetrievalQA` und `PromptTemplate` sind deprecated und wurden durch moderne Runnable-Objekte ersetzt.\n",
    "- **Runnable-Pipelines**: Anstatt Chains verwendet das Notebook jetzt die LangChain Expression Language (LCEL) mit `RunnablePassthrough`, `RunnableLambda` und dem `|` Operator f√ºr die Verkettung von Retriever, Prompt und LLM.\n",
    "- **Prompt-Template**: `PromptTemplate` (deprecated) wurde durch `ChatPromptTemplate` ersetzt.\n",
    "\n",
    "Der Funktionsumfang bleibt derselbe, aber der Code folgt nun den aktuellen Best Practices von LangChain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596dbb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installieren wir zun√§chst einige Packete die uns LangChain zur Verf√ºgung stellt. U.a. eine Schnittstelle zu Ollama.\n",
    "!uv add langchain==1.1.3 langchain-community==0.4.1 langchain_text_splitters==1.0.0 langchain-core==1.1.3 langchain-ollama==1.0.0 pypdf rank_bm25 chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbb1c38-86bf-4459-87c4-df4446d59e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f78384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4e325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_URL = \"http://132.199.138.16:11434\"\n",
    "LLM_MODEL = \"gpt-oss:20b\" \n",
    "EMBEDDING_MODEL = \"nomic-embed-text\" # Der Ollama-Server bei der Uni stellt uns ein Embedding Modell zur Verf√ºgung mit dem wir dokumente in Vektoren umwandeln k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91800a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM initialisieren\n",
    "llm = ChatOllama(\n",
    "    model=LLM_MODEL,\n",
    "    base_url=LLM_URL,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3084a",
   "metadata": {},
   "source": [
    "## 1. Dokumente laden und vorbereiten\n",
    "\n",
    "Wir nehmen als Beispiel ein PDF-Dokument zur Baugeschichte der Universit√§t Regensburg.\n",
    "\n",
    "Zun√§chst folgen wir drei Schritte:\n",
    "\n",
    "* Herunterladen der PDF von der offiziellen Webseite (auch manuell m√∂glich!)\n",
    "* Wir wandeln das PDF in einen Text um\n",
    "* Aufteilung in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbf7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pdf = \"https://www.uni-regensburg.de/fileadmin/user_upload/universitaet/archiv/Forschungshilfen/Dokumente_Unigeschichte/Infrastruktur_und_Baugeschichte/Chronik_Baugeschichte.pdf\"\n",
    "\n",
    "# download as \"ur_baugeschichte.pdf\" falls not already present\n",
    "import os, requests\n",
    "if not os.path.exists(\"ur_baugeschichte.pdf\"): \n",
    "    response = requests.get(path_pdf)\n",
    "    with open(\"ur_baugeschichte.pdf\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92d79a",
   "metadata": {},
   "source": [
    "### Dokumente laden\n",
    "\n",
    "In Langchain gibt es viele M√∂glichkeiten, Dokumente zu laden. Abh√§ngig vom Dateityp (z.B. PDF, DOCX, TXT, HTML) gibt es verschiedene Loader-Klassen. \n",
    "\n",
    "In unserem Fall verwenden wir den `PyPDFLoader`, um die PDF-Datei zu laden.\n",
    "\n",
    "Weitere Beispiele f√ºr Loader:\n",
    "\n",
    "```python\n",
    "# 1. F√ºr PDFs\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# 2. F√ºr Textdateien / Strings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "# 3. F√ºr Webseiten\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "# 4. F√ºr HTML-Dateien\n",
    "from langchain_community.document_loaders import HTMLLoader\n",
    "```\n",
    "\n",
    "Mehr Informationen zu den verschiedenen Loadern findet ihr auch in der [LangChain Dokumentation](https://docs.langchain.com/oss/javascript/integrations/document_loaders).\n",
    "\n",
    "Alle Loader-Klassen haben eine `load()`-Methode, die die Datei einliest und in eine Liste von `Document`-Objekten umwandelt. \n",
    "Jedes `Document`-Objekt enth√§lt den Textinhalt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3227c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Chunks: 32\n",
      "\n",
      "Beispiel-Chunk:\n",
      "Chronik zur universit√§ren Baugeschichte Regensburg \n",
      " \n",
      "Datum Ereignis Geb√§ude \n",
      "28.07.1964 Erteilung des Planungsauftrages f√ºr das \n",
      "Sammelgeb√§ude \n",
      " \n",
      "Quelle: RUZ 3/69, S. 8 \n",
      "Sammelgeb√§ude (RWS) \n",
      "1965 Einrichtung eines Universit√§tsbauamtes \n",
      " \n",
      "Quelle: RUZ 1/65, S. 24 \n",
      "Bauamt \n",
      "1965 Beginn der Umbauma√ünahmen am \n",
      "ehemaligen Geb√§ude des Albertus-\n",
      "Magnus-Gymnasiums (√Ñgidienplatz) f√ºr \n",
      "die vorl√§ufige Unterbringung der \n",
      "Universit√§tsbibliothek  \n",
      " \n",
      "Quelle: RUZ 1/65, S. 23-24 \n",
      "Bibliothek \n",
      "11.05.1965 Erteilung des Planungsauftrages f√ºr die \n",
      "Mensa \n",
      " \n",
      "Quelle: RUZ 3/69, S. 8 \n",
      "Mensa \n",
      "20.11.1965 Grundsteinlegung des Sammelgeb√§udes \n",
      " \n",
      "Quelle: RUZ 9/66, S. 4 \n",
      "Sammelgeb√§ude (RWS) \n",
      "1965 Ausschreibung des ersten Wettbewerbs \n",
      "f√ºr Einzelbauma√ünahmen auf Grundlage \n",
      "der strukturellen Rahmenplanung \n",
      " \n",
      "Quelle: RUZ 10/66, S. 8 \n",
      "Mensa \n",
      "28./29.01.1966 Bauwettbewerb Mensa mit Massenstudie \n",
      "f√ºr das Zentrum \n",
      " \n",
      "Erster Preis: Max D√∂mges \n",
      " \n",
      "Quelle: RUZ 1/66, S. 4; RUZ 10/66, S. 8; \n",
      "RUZ 1/78, S. 14 \n",
      "Mensa\n"
     ]
    }
   ],
   "source": [
    "# Dokument laden\n",
    "# Mehr zum Document-Objekt (https://docs.langchain.com/oss/python/integrations/document_loaders).\n",
    "loader = PyPDFLoader(\"ur_baugeschichte.pdf\")\n",
    "full_pdf = loader.load()  # load() gibt eine Liste von Document-Objekten zur√ºck, ein Document pro Seite im PDF\n",
    "\n",
    "# Text in Chunks aufteilen\n",
    "# RecursiveCharacterTextSplitter teilt lange Texte in kleinere Abschnitte (Chunks)\n",
    "# Dies ist wichtig f√ºr RAG-Systeme, da Embedding-Modelle oft L√§ngenbeschr√§nkungen haben von z:.B. 512 oder 1024 Token\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,        # Maximale L√§nge eines Chunks in Zeichen\n",
    "    chunk_overlap=50,      # √úberlappung zwischen Chunks verhindert Informationsverlust an Grenzen\n",
    "    separators=[\"\\n\", \",\", \" \", \"\"]       # Text wird prim√§r an Abs√§tzen (Doppel-Zeilenumbr√ºchen) getrennt\n",
    ")\n",
    "# split_documents() wendet das Splitting auf das Document-Objekt an\n",
    "# und gibt eine Liste von kleineren Document-Objekten zur√ºck\n",
    "docs = text_splitter.split_documents(full_pdf)\n",
    "\n",
    "# page_content enth√§lt den eigentlichen Text eines Document-Objekts\n",
    "print(f\"Anzahl der Chunks: {len(docs)}\")\n",
    "print(f\"\\nBeispiel-Chunk:\\n{docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12eeed",
   "metadata": {},
   "source": [
    "Der `RecursiveCharacterTextSplitter` versucht die Separatoren von oben nach unten in der angegebenen Reihenfolge. Er nimmt immer den ersten Separator, der den Text in Chunks `‚â§` `chunk_size` aufteilen kann. Falls ein Chunk trotzdem noch zu gro√ü ist, probiert er den n√§chsten Separator in der Liste, bis der Text klein genug ist.\n",
    "\n",
    "Der `chunk_overlap` sorgt daf√ºr, dass aufeinanderfolgende Chunks sich √ºberlappen. Die letzten X Zeichen eines Chunks werden am Anfang des n√§chsten Chunks wiederholt, damit wichtige Informationen, die √ºber Chunk-Grenzen hinausgehen, nicht verloren gehen und der semantische Kontext erhalten bleibt. [Hier](https://dev.to/tak089/what-is-chunk-size-and-chunk-overlap-1hlj)\n",
    " findet ihr eine weitere Erkl√§rung dazu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00033915",
   "metadata": {},
   "source": [
    "## 2. RAG mit Keyword-basierter Suche (BM25)\n",
    "\n",
    "BM25 ist ein klassischer Ranking-Algorithmus, der auf **Keyword-Matching** basiert. Er berechnet, wie relevant ein Dokument f√ºr eine Suchanfrage ist, basierend auf der H√§ufigkeit und Verteilung der Suchbegriffe in den Chunks. BM25 berechnet einen Relevanz-Score f√ºr jedes Dokument und sortiert sie entsprechend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdef5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 10  # Anzahl der zur√ºckgegebenen Dokumente\n",
    "\n",
    "# Beispiel-Suche mit dem BM25-Retriever\n",
    "# mit .invoke() werden die 10 relevantesten Dokumente zur√ºckgegeben\n",
    "query = \"Wann erfolgte die Inbetriebnahme des Vielberth-Geb√§udes?\"\n",
    "retrieved_docs = bm25_retriever.invoke(query)\n",
    "\n",
    "### Ggf. Auskommentieren um die gefundenen Dokumente anzuzeigen: ###\n",
    "# print(f\"Query: {query}\\n\")\n",
    "# print(\"Gefundene Dokumente:\")\n",
    "# for i, doc in enumerate(retrieved_docs):\n",
    "#     print(f\"\\n--- Dokument {i+1} ---\")\n",
    "#     print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbfa191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frage: Wann erfolgte die Inbetriebnahme des Vielberth-Geb√§udes?\n",
      "Antwort: Die Inbetriebnahme des Vielberth‚ÄëGeb√§udes erfolgte im **Mai‚ÄØ2011**.\n"
     ]
    }
   ],
   "source": [
    "# Erstellen wir eine Prompt mit ChatPromptTemplate. ChatPromptTemplate erm√∂glicht es uns, flexible Prompts mit Platzhaltern zu erstellen.\n",
    "# Die Platzhalter werden sp√§ter durch die tats√§chlichen Werte ersetzt\n",
    "template = \"\"\"Beantworte die Frage basierend auf dem folgenden Kontext:\n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Antwort:\"\"\"\n",
    "\n",
    "# Funktion zum Formatieren der abgerufenen Dokumente in einen String\n",
    "def format_docs(docs):\n",
    "    # # Alle Dokumente werden zu einem String zusammengef√ºgt, getrennt durch zwei Zeilenumbr√ºche\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Die input_variables m√ºssen mit den Platzhaltern im Template √ºbereinstimmen\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Die RAG-Pipeline f√ºr BM25:\n",
    "# 1. bm25_retriever: Holt die relevantesten Dokumente zur Frage basierend auf Keyword-Matching\n",
    "# 2. RunnableLambda(format_docs): Wandelt die Liste von Dokumenten in einen zusammenh√§ngenden String um\n",
    "# 3. RunnablePassthrough(): Leitet die Eingabe (die Frage) direkt weiter, ohne sie zu ver√§ndern. In unserem Beispiel question_1\n",
    "# 4. prompt: Setzt den formatierten Kontext und die Frage in den Prompt ein\n",
    "# 5. llm: Generiert die finale Antwort basierend auf dem Prompt. Prompt erwartet \"context\" und \"question\" als Eingaben, daher das Dictionary am Anfang.\n",
    "# \"|\" : Verkettet Runnable-Objekte in der LangChain Expression Language (LCEL), wobei die Ausgabe des linken Objekts die Eingabe des rechten wird.\n",
    "qa_chain_bm25 = (\n",
    "    {\"context\": bm25_retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Fragen stellen\n",
    "question_1 = \"Wann erfolgte die Inbetriebnahme des Vielberth-Geb√§udes?\"\n",
    "result = qa_chain_bm25.invoke(question_1) # Mit .invoke() wird die Kette ausgef√ºhrt\n",
    "print(f\"Frage: {question_1}\")\n",
    "print(f\"Antwort: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f47c8e",
   "metadata": {},
   "source": [
    "## 3. RAG mit Embeddings (Semantische Suche)\n",
    "\n",
    "Bei der **Embedding-basierten Suche** werden Texte in numerische Vektoren umgewandelt, die ihre semantische Bedeutung repr√§sentieren. \n",
    "\n",
    "Wir verwenden ChromaDB (oft als \"Chroma\" bezeichnet), eine Open-Source Vektor-Datenbank. Jedem Vektor wird ein Text-Chunk zugeordnet, sodass wir bei einer Suchanfrage die semantisch √§hnlichsten Chunks finden k√∂nnen.\n",
    "\n",
    "Mit `vectorstore.get()` k√∂nnten wir alle Vektoren abrufen als Python-Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2adfa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Was passierte 1980 an der Universit√§t Regensburg?\n",
      "\n",
      "Gefundene Dokumente:\n",
      "\n",
      "--- Dokument 1 ---\n",
      "Quelle: RUZ 7/79, S. 2 \n",
      "10.12.1979 Das Universit√§tsbauamt legt zwei \n",
      "unterschiedliche Ausbauvarianten der \n",
      "Mensa als Haushaltsunterlage HU-Bau \n",
      "zur Pr√ºfung vor, die beide von einer \n",
      "baulichen Erweiterung des \n",
      "Mensageb√§udes nach Westen ausgingen   \n",
      "Mensa \n",
      "1980 Fertigstellung des Botanischen Versuchs- \n",
      "und Lehrgartens der Universit√§t \n",
      " \n",
      "Quelle: RUZ 3/82, S. 7 \n",
      "Botanischer Garten \n",
      "20.05.1980 Genehmigung eines \n",
      "Erg√§nzungsprogramms und Erteilung des \n",
      "Planungsauftrags f√ºr den Fachbereich \n",
      "Chemie Pharmazie \n",
      "Chemie Pharmazie \n",
      "22.07.1980 Richtfest der ersten Baustufe des \n",
      "Klinikums (ZMK) \n",
      " \n",
      "Quelle: RUZ 6/80, S. 2 \n",
      "Klinikum \n",
      "1983 Die UR ist Schauplatz f√ºr die ARD-\n",
      "Fernsehserie ‚ÄûMatt in 13 Z√ºgen‚Äú, die ab \n",
      "1984 ausgestrahlt wird (v.a. die R√§ume \n",
      "von Prof. Dr. M√§rkl/Chemie u. die \n",
      "Pizzeria). (RUZ 1983) Darsteller waren \n",
      "u.a. Gudrun Landgrebe, Mathieu \n",
      "Carri√®re, Tommy Piper und Peer \n",
      "Augustinski. \n",
      "Chemie \n",
      "02.11.1983 Die Zahn-, Mund- und Kieferklinik wurde \n",
      "der Universit√§t Regensburg √ºbergeben \n",
      "Klinikum\n"
     ]
    }
   ],
   "source": [
    "# Embeddings-Modell initialisieren.\n",
    "# Auf dem Ollama-Server der Uni ist das Modell \"nomic-embed-text\" verf√ºgbar, mit dem wir Texte in Vektoren umwandeln k√∂nnen.\n",
    "# Wichtige Hinweise: \n",
    "# (1) Die Umwandlung in Vekotren braucht Zeit und Rechenressourcen. Je nach Dokumentenl√§nge kann dies mehrere Minuten dauern, also bitte Geduld haben bzw. Daten gut vorbereiten!\n",
    "# (2) Das Embedding-Modell hat eine L√§ngenbeschr√§nkung von ca. 512 Token. L√§ngere Texte m√ºssen vorher in Chunks aufgeteilt werden!\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=LLM_URL\n",
    ")\n",
    "\n",
    "try:\n",
    "  # Dieses Codest√ºck habe ich hinzugef√ºgt, um sicherzustellen, dass bei wiederholtem Ausf√ºhren des Notebooks kein doppelter Vektor-Speicher entstehen.\n",
    "  vectorstore.delete_collection() \n",
    "except:\n",
    "  pass\n",
    "\n",
    "# Mit Chroma einen Vektor-Speicher aus den Dokumenten erstellen.\n",
    "vectorstore = Chroma.from_documents(docs, ollama_embeddings)\n",
    "\n",
    "# Retriever erstellen. Der Parameter k gibt an, wie viele √§hnliche Dokumente zur√ºckgegeben werden sollen.\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Beispiel-Suche\n",
    "query = \"Was passierte 1980 an der Universit√§t Regensburg?\"\n",
    "retrieved_docs = vector_retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Gefundene Dokumente:\")\n",
    "for i, doc in enumerate(retrieved_docs[:1]):\n",
    "    print(f\"\\n--- Dokument {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4671e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frage: Was passierte 1980 an der Universit√§t Regensburg?\n",
      "Antwort: Im Jahr‚ÄØ1980 gab es mehrere bedeutende Bau- und Planungsereignisse an der Universit√§t Regensburg:\n",
      "\n",
      "| Datum | Ereignis |\n",
      "|-------|----------|\n",
      "| **20.‚ÄØMai‚ÄØ1980** | Genehmigung eines Erg√§nzungsprogramms und Erteilung des Planungsauftrags f√ºr den Fachbereich Chemie‚ÄØ/‚ÄØPharmazie. |\n",
      "| **22.‚ÄØJuli‚ÄØ1980** | Richtfest der ersten Baustufe des Klinikums (Zahn‚Äë, Mund‚Äë und Kieferklinik ‚Äì ZMK). |\n",
      "| **1980 (Gesamt)** | Fertigstellung des Botanischen Versuchs‚Äë und Lehrgartens der Universit√§t. |\n",
      "\n",
      "Kurz gesagt: 1980 wurde der Botanische Garten fertiggestellt, ein erg√§nzendes Programm f√ºr Chemie‚ÄØ/‚ÄØPharmazie genehmigt und ein Planungsauftrag erteilt, und die erste Baustufe des Klinikums erhielt ihr Richtfest.\n"
     ]
    }
   ],
   "source": [
    "# Wir k√∂nnen nun eine RAG-Chain mit dem Embedding-basierten Retriever erstellen.\n",
    "qa_chain_embeddings = (\n",
    "    {\"context\": vector_retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Frage stellen\n",
    "result = qa_chain_embeddings.invoke(query)\n",
    "print(f\"Frage: {query}\")\n",
    "print(f\"Antwort: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1999aa",
   "metadata": {},
   "source": [
    "## √úbung: Daten aus mehreren Textdateien laden\n",
    "\n",
    "In der Praxis m√∂chte man oft nicht nur ein einzelnes Dokument verwenden, sondern eine ganze Sammlung von Textdateien als Wissensbasis f√ºr RAG nutzen.\n",
    "Recherchiert, wie man mit `DirectoryLoader` mehrere Textdateien aus einem Verzeichnis laden kann.\n",
    "\n",
    "√úber GRIPS habe ich euch einige Textdateien zur Verf√ºgung gestellt, die ihr f√ºr RAG verwenden k√∂nnt.\n",
    "Es handelt sich um kurze Berichte zu Sonnenfinsternissen in den kommenden Jahrzehnten.\n",
    "\n",
    "So k√∂nnt ihr eine RAG-Pipeline mit diesen Textdateien erstellen:\n",
    "\n",
    "* 1. Ladet alle Textdateien aus `datenbank/` mit `DirectoryLoader`\n",
    "* 2. Erstellt einen Vektor-Speicher mit Chroma und Ollama-Embeddings\n",
    "* 3. Verwendet wieder einen `RecursiveCharacterTextSplitter`, um die Dokumente in Chunks aufzuteilen. Da unser Embedding-Modell eine maximale Eingabel√§nge von 512 Tokens hat, setzen wir die `chunk_size` auf 500 und `chunk_overlap` auf 50, um sicherzugehen, dass die Chunks die L√§ngenbeschr√§nkung einhalten.\n",
    "* 4. Erstellt einen `vector_retriever`, der die 5 semantisch √§hnlichsten Chunks f√ºr eine Suchanfrage zur√ºckgibt.\n",
    "* 5. Testet den `vector_retriever` mit einer Beispiel-Suchanfrage, z.B. \"Wann gibt es bei den Polargebieten eine totale Sonnenfinsternis?\"\n",
    "* 6. Erstellt eine RAG-Chain mit dem Embedding-basierten Retriever und testet sie mit der gleichen Suchanfrage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c13ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Code einf√ºgen..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a2264",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>L√∂sung anzeigen</b></summary>\n",
    "\n",
    "<p>Dokumente aus mehreren Textdateien laden:</p>\n",
    "<br/>\n",
    "-------\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"datenbank/\",\n",
    "    glob=\"*.txt\", # Nur .txt Dateien laden\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# Wichtig!: Zwar sind die Dokumente in der Datenbank meist schon kurz, aber wir teilen sie dennoch in Chunks auf, um sicherzugehen,\n",
    "# dass sie die L√§ngenbeschr√§nkungen der Embedding-Modelle einhalten.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\", \",\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "vectorstore.delete_collection()\n",
    "vectorstore = Chroma.from_documents(docs, ollama_embeddings)\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "```\n",
    "\n",
    "<p>Testen des `vector_retriever` mit einer Beispiel-Suchanfrage:</p>\n",
    "<br/>\n",
    "-------\n",
    "\n",
    "```python\n",
    "# Beispiel-Suche\n",
    "query = \"Wann gibt es bei den Polargebieten eine totale Sonnenfinsternis?\"\n",
    "retrieved_docs = vector_retriever.invoke(query)\n",
    "\n",
    "\n",
    "qa_chain_embeddings = (\n",
    "    {\"context\": vector_retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Frage stellen\n",
    "result = qa_chain_embeddings.invoke(query)\n",
    "print(f\"Antwort: {result.content}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
