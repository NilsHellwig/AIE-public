{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edb5de3",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Notebook: Einf√ºhrung in Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "In diesem Notebook lernen wir, wie wir **Retrieval-Augmented Generation (RAG)** nutzen k√∂nnen, um die Antworten von gro√üen Sprachmodellen (LLMs) durch externe Wissensquellen zu verbessern. Wir werden eine einfache Anwendung erstellen, die Fragen zu einem gegebenen Text beantwortet.\n",
    "\n",
    "## üìö Quellen\n",
    "\n",
    "- [LangChain Dokumentation zu RAG](https://python.langchain.com/docs/tutorials/rag/)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Viel Erfolg beim Ausprobieren und Validieren! ü§ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596dbb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installieren wir zun√§chst einige Packete die uns LangChain zur Verf√ºgung stellen. U.a. eine Schnittstelle zu Ollama.\n",
    "!pip install langchain langchain-community langchain-ollama rank-bm25 chromadb ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f78384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_URL = \"http://132.199.138.16:11434\"\n",
    "LLM_MODEL = \"gemma3:12b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\" # Unser Ollama-Server bei der Uni stellt uns ein Embedding Modell zur Verf√ºgung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91800a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM initialisieren\n",
    "llm = ChatOllama(\n",
    "    model=LLM_MODEL,\n",
    "    base_url=LLM_URL,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3084a",
   "metadata": {},
   "source": [
    "## 1. Dokumente laden und vorbereiten\n",
    "\n",
    "Zun√§chst laden wir unser Wissens-Dokument und teilen es in kleinere Chunks auf. Diese Chunks sind die Basis f√ºr die sp√§tere Suche.\n",
    "\n",
    "Bei dem Dokument handelt es sich um eine Publikation von unserem Lehrstuhl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dokument laden\n",
    "# TextLoader ist eine LangChain-Klasse zum Einlesen von Textdateien\n",
    "# Sie liest die gesamte Datei ein und erstellt daraus ein Langchain Document-Objekt (https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).\n",
    "# encoding=\"utf-8\" stellt sicher, dass Umlaute und Sonderzeichen korrekt gelesen werden\n",
    "loader = TextLoader(\"dummy_wissen.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()  # load() gibt eine Liste von Document-Objekten zur√ºck, in diesem Fall mit nur einem Element\n",
    "\n",
    "# Text in Chunks aufteilen\n",
    "# RecursiveCharacterTextSplitter teilt lange Texte in kleinere Abschnitte (Chunks)\n",
    "# Dies ist wichtig f√ºr RAG-Systeme, da Embedding-Modelle oft L√§ngenbeschr√§nkungen haben von z:.B. 512 oder 1024 Token\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximale L√§nge eines Chunks in Zeichen\n",
    "    chunk_overlap=50,      # √úberlappung zwischen Chunks verhindert Informationsverlust an Grenzen\n",
    "    separators=[\"\\n\", \",\", \" \", \"\"]       # Text wird prim√§r an Abs√§tzen (Doppel-Zeilenumbr√ºchen) getrennt\n",
    ")\n",
    "# split_documents() wendet das Splitting auf das Document-Objekt an\n",
    "# und gibt eine Liste von kleineren Document-Objekten zur√ºck\n",
    "docs = text_splitter.split_documents(document)\n",
    "\n",
    "# page_content enth√§lt den eigentlichen Text eines Document-Objekts\n",
    "print(f\"Anzahl der Chunks: {len(docs)}\")\n",
    "print(f\"\\nBeispiel-Chunk:\\n{docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12eeed",
   "metadata": {},
   "source": [
    "Der `RecursiveCharacterTextSplitter` versucht die Separatoren von oben nach unten in der angegebenen Reihenfolge. Er nimmt immer den ersten Separator, der den Text in Chunks `‚â§` `chunk_size` aufteilen kann. Falls ein Chunk trotzdem noch zu gro√ü ist, probiert er den n√§chsten Separator in der Liste, bis der Text klein genug ist.\n",
    "\n",
    "Der `chunk_overlap` sorgt daf√ºr, dass aufeinanderfolgende Chunks sich √ºberlappen. Die letzten X Zeichen eines Chunks werden am Anfang des n√§chsten Chunks wiederholt, damit wichtige Informationen, die √ºber Chunk-Grenzen hinausgehen, nicht verloren gehen und der semantische Kontext erhalten bleibt. Mehr zu dem Thema Text-Splitting [hier](https://dev.to/tak089/what-is-chunk-size-and-chunk-overlap-1hlj#:~:text=Improves%20Retrieval%3A%20Overlapping%20chunk%20helps,smaller%20pieces%2C%20allowing%20efficient%20processing.)\n",
    "\n",
    "```python\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00033915",
   "metadata": {},
   "source": [
    "## 2. RAG mit Keyword-basierter Suche (BM25)\n",
    "\n",
    "BM25 ist ein klassischer Ranking-Algorithmus, der auf **Keyword-Matching** basiert. Er berechnet, wie relevant ein Dokument f√ºr eine Suchanfrage ist, basierend auf der H√§ufigkeit und Verteilung der Suchbegriffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Retriever erstellen\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2  # Anzahl der zur√ºckgegebenen Dokumente\n",
    "\n",
    "# Beispiel-Suche mit dem BM25-Retriever\n",
    "# mit .invoke() werden die 2 relevantesten Dokumente zur√ºckgegeben\n",
    "query = \"Wann beginnt das Wintersemester?\"\n",
    "retrieved_docs = bm25_retriever.invoke(query)\n",
    "\n",
    "# print(f\"Query: {query}\\n\")\n",
    "# print(\"Gefundene Dokumente:\")\n",
    "# for i, doc in enumerate(retrieved_docs):\n",
    "#     print(f\"\\n--- Dokument {i+1} ---\")\n",
    "#     print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbfa191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen wir eine promot mit PromptTemplate\n",
    "# Das Template enth√§lt Platzhalter f√ºr Kontext und Frage\n",
    "# Diese werden sp√§ter durch die tats√§chlichen Werte ersetzt\n",
    "template = \"\"\"Beantworte die Frage basierend auf dem folgenden Kontext:\n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Antwort:\"\"\"\n",
    "\n",
    "# Die input_variables m√ºssen mit den Platzhaltern im Template √ºbereinstimmen\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "qa_chain_bm25 = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=bm25_retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Frage stellen\n",
    "question = \"Wer neben Christian Wolff hat das Paper geschrieben?\"\n",
    "result = qa_chain_bm25.invoke({\"query\": question})\n",
    "print(f\"Frage: {question}\")\n",
    "print(f\"Antwort: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f47c8e",
   "metadata": {},
   "source": [
    "## 3. RAG mit Embeddings (Semantische Suche)\n",
    "\n",
    "Bei der **Embedding-basierten Suche** werden Texte in numerische Vektoren umgewandelt, die ihre semantische Bedeutung repr√§sentieren. \n",
    "\n",
    "Wir verwenden ChromaDB (oft als \"Chroma\" bezeichnet), eine Open-Source Vector Datenbank. ChromaDB speichert Veektore effizient und erm√∂glicht schnelle √Ñhnlichkeitssuchen.\n",
    "\n",
    "Mit `vectorstore.get()` k√∂nnten wir alle Vektoren abrufen als Python-Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embeddings-Modell initialisieren\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=LLM_URL\n",
    ")\n",
    "\n",
    "# Mit Chroma einen Vektor-Speicher aus den Dokumenten erstellen\n",
    "vectorstore = Chroma.from_documents(docs, ollama_embeddings)\n",
    "\n",
    "# Retriever erstellen. Der Parameter k gibt an, wie viele √§hnliche Dokumente zur√ºckgegeben werden sollen\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Beispiel-Suche\n",
    "query = \"Was ist die E-Mail Addresse von Udo Kruschwitz?\"\n",
    "retrieved_docs = vector_retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Gefundene Dokumente:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Dokument {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Chain mit Embeddings erstellen\n",
    "qa_chain_embeddings = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Frage stellen\n",
    "result = qa_chain_embeddings.invoke({\"query\": query})\n",
    "print(f\"Frage: {query}\")\n",
    "print(f\"Antwort: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1999aa",
   "metadata": {},
   "source": [
    "### √úbungsaufgabe: Daten aus CSV\n",
    "\n",
    "Implementiere ein Retrieval-Augmented Generation (RAG) System, das Fragen zu den YouTube Top 100 Songs 2025 beantwortet.\n",
    "\n",
    "Wir k√∂nnen nach folgenden Schritten vorgehen:\n",
    "\n",
    "1. Daten laden: Lade die CSV-Datei `youtube-top-100-songs-2025.csv` mit dem CSVLoader\n",
    "2. Chunking: Teile die Dokumente in Chunks (max. 500 Zeichen, 50 Zeichen √úberlappung)\n",
    "3. Embeddings & Vector Store: Erstelle Embeddings mit Ollama und speichere sie in einer ChromaDB\n",
    "4. Retriever: Konfiguriere einen Retriever, der die 5 relevantesten Chunks findet\n",
    "5. RAG-Chain: Erstelle eine RetrievalQA-Chain mit einem Custom-Prompt-Template\n",
    "6. Testen: Beantworte z.B. die Frage: \"Wie lange ist das Musikvideo zu 'ALL MY LOVE' von Coldplay?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier kann der Code geschrieben werden..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671eb278",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>L√∂sung anzeigen</b></summary>\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# Laden wir nun mit CSVLoader die YouTube Top 100 Songs 2025 CSV-Datei\n",
    "# Hinweis: Oft kann es Sinn machen, die CSV-Datei noch vorzubereiten, z.B. irrelevante Spalten zu entfernen\n",
    "# oder fehlende Werte zu erg√§nzen. Erinnerung: Dies kann z.B. mit Pandas gemacht werden.\n",
    "loader = CSVLoader(\n",
    "    file_path=\"youtube-top-100-songs-2025.csv\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# In Chunks aufteilen\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\", \",\", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Embeddings erzeugen und tempor√§re Chroma-DB erstellen\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=LLM_URL\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=ollama_embeddings\n",
    ")\n",
    "\n",
    "# Retriever definieren\n",
    "vector_retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 5})  # 5 relevante Chunks abrufen\n",
    "\n",
    "# Beispiel-Suche\n",
    "query = \"Wie lange ist das Musikvideo zu 'ALL MY LOVE' von Coldplay?\"\n",
    "\n",
    "# Relevante Dokumente abrufen\n",
    "retrieved_docs = vector_retriever.invoke(query)\n",
    "\n",
    "template = \"\"\"Beantworte zu den top 100 YouTube Musikvideos die Frage basierend auf dem folgenden Kontext:\n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Antwort:\"\"\"\n",
    "\n",
    "# Die input_variables m√ºssen mit den Platzhaltern im Template √ºbereinstimmen\n",
    "prompt = PromptTemplate(template=template, input_variables=[\n",
    "                        \"context\", \"question\"])\n",
    "\n",
    "# RAG-Chain mit Embeddings erstellen\n",
    "qa_chain_embeddings = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# Frage stellen\n",
    "result = qa_chain_embeddings.invoke({\"query\": query})\n",
    "print(f\"Frage: {query}\")\n",
    "print(f\"Antwort: {result['result']}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nils_hellwig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
