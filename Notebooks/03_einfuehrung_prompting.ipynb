{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40f98b0",
   "metadata": {},
   "source": [
    "# Notebook: Einf√ºhrung inPrompting-Strategien f√ºr Large Language Models (LLMs)\n",
    "\n",
    "In diesem Notebook werden verschiedene Prompting-Strategien vorgestellt, die helfen k√∂nnen, die Leistung von Large Language Models (LLMs) zu verbessern.\n",
    "\n",
    "## üìö Quellen\n",
    "\n",
    "- [Prompting Guide](https://www.promptingguide.ai/de/techniques)\n",
    "- [Python-Paket `openai`](https://github.com/openai/openai-python)\n",
    "\n",
    "---\n",
    "\n",
    "Viel Spa√ü beim Erkunden und Experimentieren! ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dba87b",
   "metadata": {},
   "source": [
    "Wir verwenden das `openai`-Paket, um mit einem LLM zu interagieren. Das Paket erlaubt sowohl die Kommunikation mit der OpenAI-API als auch mit anderen LLMs, die eine kompatible API anbieten. \n",
    "\n",
    "**Wichtiger Hinweis:** Bei der Uni haben wir mehrere Large Language Models zur Verf√ºgung gestellt √ºber einen Server. Ich verweise an dieser Stelle auf die Anleitung f√ºr den VPN-Zugang, den ich bei GRIPS hinterlegt habe. Anfragen an das LLM sind nur **innerhalb des Uni-Netzwerks** m√∂glich, das bedeutet, dass ihr entweder auf dem Campus seid im `eduroam` oder √ºber VPN verbunden seid.\n",
    "\n",
    "Auf dem Uni-Server werden mehrere Open-Source LLMs angeboten, wobei diese ollama bereitgestellt werden.\n",
    "\n",
    "Wir werden nun das Packet installieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c1349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff304e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren wir nun das Paket\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3637272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren wir die API-URL und den API-Schl√ºssel. Beachte den Hinweis oben!\n",
    "LLM_URL = \"http://132.199.138.16:11434/v1\"\n",
    "LLM_MODEL = \"gemma3:4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d03b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=LLM_URL,\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef581dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren wir eine Funktion, die eine Anfrage an das LLM stellt. Dabei handelt es sich um die completions-API, die eine Next-Token-Vorhersage durchf√ºhrt.\n",
    "# Es werden maximal `max_tokens` generiert, die durch das `stop`-Token begrenzt werden k√∂nnen.\n",
    "def llm_completion(prompt, temperature=0.0, stop=[\"\\n\"], seed=0, max_tokens=256):\n",
    "    completion = client.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        temperature=temperature,\n",
    "        stop=stop,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        seed=seed\n",
    "    )\n",
    "    return completion.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bfcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren wir noch eine weitere Funktion, die die Chat-Completions-API nutzt. Diese erlaubt es, Nachrichten in einem Chat-Format zu √ºbergeben.\n",
    "def llm_completion_chat(prompt, temperature=0.0, stop=None, image_url=None, seed=0):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "            ] + ([{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}] if image_url else [])\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    extra_body = {\"stop\": [stop]} if stop else {}\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        stream=False,\n",
    "        seed=seed,\n",
    "        extra_body=extra_body,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fd639",
   "metadata": {},
   "source": [
    "## 1. Zero-shot Prompting\n",
    "\n",
    "**Zero-shot Prompting** bedeutet, dass Sie dem LLM direkt eine Aufgabe stellen ‚Äì ganz ohne Beispiele oder zus√§tzliche Erkl√§rungen.  \n",
    "\n",
    "Das Modell nutzt sein trainiertes Wissen, um Ihre Anfrage bestm√∂glich zu beantworten. Oft funktioniert das √ºberraschend gut, besonders bei klar formulierten Fragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff7bf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris ist die Hauptstadt von Frankreich.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Paris ist die Hauptstadt von?\"\n",
    "print(llm_completion_chat(prompt, stop=\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca4eaf",
   "metadata": {},
   "source": [
    "## 2. Few-shot Prompting\n",
    "\n",
    "**Few-shot Prompting** bedeutet, dass Sie dem LLM einige Beispiele geben, um mehr Kontext zu bieten.\n",
    "\n",
    "Dies hilft dem Modell, Ihre Anfrage besser zu verstehen und relevantere Antworten zu generieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55893e7c",
   "metadata": {},
   "source": [
    "In dem Kontext ist das Paper **‚ÄûLanguage Models are Few-Shot Learners‚Äú** von Brown et al. (2020), das 2020 GPT-3 vorgestellt hatte von besonderer Bedeutung. Statt das Modell neu zu trainieren, reicht es bei LLMs, dem Modell wenige Beispiele direkt im Prompt zur Verf√ºgung zu stellen (Few-Shot), um neue Aufgaben zu l√∂sen. Es kann in gewisser Weise von einem Paradigmenwechsel gesprochen werden: Statt f√ºr jede Aufgabe ein Modell feinzujustieren, k√∂nnen gro√üe Modelle durch Prompt-Engineering generalisieren.\n",
    "\n",
    "[Hier geht es zum Paper](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c661ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Elemente: [(\"Stadt\", \"positive\")]\n"
     ]
    }
   ],
   "source": [
    "# In dem folgenden Beispiel wird ein Prompt verwendet, um Sentiment-Elemente aus S√§tzen zu extrahieren.\n",
    "# Das Modell soll die relevanten Elemente identifizieren und deren Sentiment klassifizieren.\n",
    "prompt = '''Satz: Es war richtig toll in Berlin.\n",
    "Sentiment Elemente: [(\"Berlin\", \"positive\")]\n",
    "Satz: Das Essem war nicht so lecker.\n",
    "Sentiment Elemente: [(\"Essen\", \"negative\")]\n",
    "Satz: Hier in Portugal gibt es fantastische Str√§nde aber leider war das Wetter schlecht.\n",
    "Sentiment Elemente: [(\"Str√§nde\", \"positive\"), (\"Wetter\", \"negative\")]\n",
    "Satz: Die Stadt ist sehr sch√∂n.\n",
    "Sentiment Elemente: '''\n",
    "\n",
    "# stop ist ein optionales Stop-Kriterium, um die Antwort zu beenden\n",
    "print(llm_completion(prompt, stop=\"\\n\"))  # Stoppt die Antwort bei \"Satz:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e495d90",
   "metadata": {},
   "source": [
    "### 3. Chain of Thought\n",
    "\n",
    "Eingef√ºhrt in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), erm√∂glicht Chain-of-Thought (CoT) Prompting komplexe Schlussfolgerungsf√§higkeiten durch Zwischenschritte im Denkprozess. Sie k√∂nnen es mit Few-Shot-Prompting kombinieren, um bessere Ergebnisse bei komplexeren Aufgaben zu erzielen, die eine Schlussfolgerung vor der Beantwortung erfordern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd4ba106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, lass uns das Schritt f√ºr Schritt durchgehen:\n",
      "\n",
      "1. **Start:** Du hast mit 10 √Ñpfeln angefangen.\n",
      "2. **An den Nachbarn:** Du hast 2 √Ñpfel gegeben, also bleiben 10 - 2 = 8 √Ñpfel.\n",
      "3. **An den Handwerker:** Du hast 2 √Ñpfel gegeben, also bleiben 8 - 2 = 6 √Ñpfel.\n",
      "4. **Neue K√§ufe:** Du kaufst 5 √Ñpfel dazu, also hast du 6 + 5 = 11 √Ñpfel.\n",
      "5. **Du isst einen Apfel:** Du isst 1 Apfel, also bleiben 11 - 1 = 10 √Ñpfel.\n",
      "\n",
      "**Antwort:** Du hast 10 √Ñpfel √ºbrig.\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Ich ging auf den Markt und kaufte 10 √Ñpfel. Ich gab 2 √Ñpfel an den Nachbarn und 2 an den Handwerker. Dann ging ich und kaufte 5 weitere √Ñpfel und a√ü 1. Wie viele √Ñpfel blieben mir √ºbrig?\n",
    "Lass uns Schritt f√ºr Schritt denken.\\n'''\n",
    "\n",
    "print(llm_completion_chat(prompt))  # Stoppt die Antwort bei \"Lass uns Schritt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be17c4d",
   "metadata": {},
   "source": [
    "## 4. Self-Consistency / Majority Vote\n",
    "\n",
    "Self-Consistency ist eine Technik, die in [Wang et al. (2022)](https://arxiv.org/abs/2203.11171) eingef√ºhrt wurde. Sie nutzt die Tatsache, dass LLMs bei der Beantwortung von Fragen oft mehrere plausible Antworten generieren k√∂nnen. Statt sich auf eine einzelne Antwort zu verlassen, aggregiert Self-Consistency mehrere Antworten und w√§hlt die h√§ufigste Antwort aus, um die Genauigkeit zu erh√∂hen.\n",
    "\n",
    "In dem Beispiel unten generieren wir mehrere Antworten, indem wir die `temperature` auf 0.8 setzen, und w√§hlen dann die h√§ufigste Antwort aus. Ich erinnere an die Vorlesung, wobei wir die `temperature` auf 0 gesetzt hatten, um eine deterministische Antwort zu erhalten, 0.8 sorgt f√ºr mehr Vielfalt in den Antworten, da nicht das Token mit der h√∂chsten Wahrscheinlichkeit immer ausgew√§hlt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc5850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9, 9, 9, 9, 9], 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "prompt = '''Q: Anna hat 2 √Ñpfel. Sie bekommt 3 weitere √Ñpfel von ihrer Freundin. Wie viele √Ñpfel hat sie jetzt?\n",
    "A: Anna hat 2 √Ñpfel. Sie bekommt 3 weitere. 2 + 3 = 5. Die Antwort ist 5.\n",
    "\n",
    "Q: Tom hat 7 Buntstifte. Er kauft 5 weitere. Wie viele Buntstifte hat er insgesamt?\n",
    "A: Tom hat 7 Buntstifte. Er kauft 5 weitere. 7 + 5 = 12. Die Antwort ist 12.\n",
    "\n",
    "Q: Lisa hat 10 Euro. Sie bekommt 4 Euro Taschengeld. Wie viel Geld hat sie danach?\n",
    "A: Lisa hat 10 Euro. Sie bekommt 4 Euro dazu. 10 + 4 = 14. Die Antwort ist 14.\n",
    "\n",
    "Q: Paul hat 3 Tafeln Schokolade. Er bekommt von seinem Bruder 6 Tafeln Schokolade dazu. Wie viel Schokolade hat Paul?\n",
    "A: '''\n",
    "\n",
    "# Wir k√∂nnen nun Self-Consistency aufrufen\n",
    "predictions = [llm_completion(prompt, stop=\"\\n\\n\", seed=i, temperature=0.8) for i in range(5)]\n",
    "\n",
    "prediction_ints = []\n",
    "for p in predictions:\n",
    "    numbers = re.findall(r'\\d+', p)\n",
    "    if numbers:\n",
    "        # Wir nehmen die letzte gefundene Zahl als Vorhersage\n",
    "        prediction_ints.append(int(numbers[-1]))\n",
    "    else:\n",
    "        prediction_ints.append(None)\n",
    "\n",
    "# Ausgabe: f√ºnf Vorhersagen + h√§ufigste Antwort\n",
    "prediction_ints, max(set(prediction_ints), key=prediction_ints.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d293cb7",
   "metadata": {},
   "source": [
    "## 5. Generiertes Wissens-Prompting\n",
    "\n",
    "Gro√üe Sprachmodelle (LLMs) werden kontinuierlich verbessert, und eine beliebte Technik beinhaltet die F√§higkeit, Wissen oder Informationen einzubinden, um dem Modell zu helfen, genauere Vorhersagen zu treffen.\n",
    "\n",
    "Kann das Modell mit einer √§hnlichen Idee auch genutzt werden, um Wissen zu generieren, bevor eine Vorhersage getroffen wird? Genau das wird im Paper von [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf) versucht ‚Äì Wissen zu generieren, das als Teil des Prompts verwendet wird. Wie n√ºtzlich ist dies f√ºr Aufgaben wie Schlussfolgerungen nach gesundem Menschverstand?\n",
    "\n",
    "Schritte:\n",
    "\n",
    "1. Zuerst generieren wir einige \"Wissensst√§nde\":\n",
    "2. Dann verwenden wir diese Wissensst√§nde, um den Prompt zu erweitern.\n",
    "3. Schlie√ülich verwenden wir den erweiterten Prompt, um die Vorhersage zu treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c31e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frage: Ist die Erde ist der einzige Planet im Sonnensystem?\"\n",
      "Wissen: Wissen: Es gibt acht Planeten in unserem Sonnensystem: Merkur, Venus, Erde, Mars, Jupiter, Saturn, Uranus und Neptun.\n",
      "Antwort:Nein, die Erde ist nicht der einzige Planet im Sonnensystem. Es gibt acht Planeten: Merkur, Venus, Erde, Mars, Jupiter, Saturn, Uranus und Neptun.\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Eingabe: Griechenland ist gr√∂√üer als Mexiko.\n",
    "Wissen: Griechenland ist ungef√§hr 131.957 Quadratkilometer gro√ü, w√§hrend Mexiko ungef√§hr 1.964.375 Quadratkilometer gro√ü ist. Mexiko ist daher 1.389% gr√∂√üer als Griechenland.\n",
    "\n",
    "Eingabe: Brillen beschlagen immer.\n",
    "Wissen: Kondensation tritt auf Brillengl√§sern auf, wenn Wasserdampf aus Ihrem Schwei√ü, Atem und der umgebenden Feuchtigkeit auf eine kalte Oberfl√§che trifft, abk√ºhlt und sich dann in winzige Fl√ºssigkeitstr√∂pfchen verwandelt und einen Film bildet, den Sie als Beschlag wahrnehmen. Ihre Gl√§ser werden im Vergleich zu Ihrem Atem relativ k√ºhl sein, besonders wenn die Au√üenluft kalt ist.\n",
    "\n",
    "Eingabe: Ein Fisch ist in der Lage zu denken.\n",
    "Wissen: Fische sind intelligenter, als sie scheinen. In vielen Bereichen, wie beispielsweise dem Ged√§chtnis, stehen ihre kognitiven F√§higkeiten denen von 'h√∂heren' Wirbeltieren, einschlie√ülich nichtmenschlicher Primaten, in nichts nach. Die Langzeitged√§chtnisse der Fische helfen ihnen, komplexe soziale Beziehungen im √úberblick zu behalten.\n",
    "\n",
    "Eingabe: Eine h√§ufige Wirkung des Rauchens vieler Zigaretten im Laufe des Lebens ist eine √ºberdurchschnittlich hohe Wahrscheinlichkeit, Lungenkrebs zu bekommen.\n",
    "Wissen: Diejenigen, die konstant weniger als eine Zigarette pro Tag im Laufe ihres Lebens geraucht haben, hatten ein neunmal h√∂heres Risiko an Lungenkrebs zu sterben als Nichtraucher. Bei Personen, die zwischen einer und 10 Zigaretten pro Tag rauchten, war das Risiko an Lungenkrebs zu sterben fast 12 Mal h√∂her als bei Nichtrauchern.\n",
    "\n",
    "Eingabe: Wie viele Planeten gibt es im Sonnensystem?\n",
    "Wissen: '''\n",
    "\n",
    "wissen = llm_completion(prompt, stop=\"\\n\")\n",
    "\n",
    "prompt_mit_wissen = f'''Frage: Ist die Erde ist der einzige Planet im Sonnensystem?\"\n",
    "Wissen: {wissen}\n",
    "Antwort:'''\n",
    "\n",
    "antwort = llm_completion(prompt_mit_wissen, stop=\"\\n\")\n",
    "print(prompt_mit_wissen + antwort)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f87d40",
   "metadata": {},
   "source": [
    "### Aufgabe 1: Named Entity Recognition mit Few-Shot Learning\n",
    "\n",
    "#### Ziel der Aufgabe\n",
    "Entwickeln Sie eine Few-Shot-Prompt-Strategie zur automatischen Erkennung von **Ortsnamen** (LOC - Location Entities) in deutschen Texten.  \n",
    "Als Datengrundlage dient die vorbereitete Datei **`LOC_sentences.txt`**, die S√§tze im Format  \n",
    "\n",
    "```\n",
    "Satz####[\"Entity1\", \"Entity2\", ...]\n",
    "```\n",
    "\n",
    "enth√§lt. Jeder Eintrag besteht also aus einem deutschen Satz und der zugeh√∂rigen Liste aller Ortsnamen (LOC-Entities) in diesem Satz.  \n",
    "\n",
    "#### Anforderungen\n",
    "1. **Few-Shot Learning:** Erstellen Sie eine Prompt mit **f√ºnf zuf√§lligen Few-shot Beispielen** aus `LOC_sentences.txt`, die das gew√ºnschte Input-Output-Verhalten demonstrieren.  \n",
    "2. **Ausgabeformat:** Das Modell soll erkannte Ortsnamen als String-Liste zur√ºckgeben (z. B. `[\"Berlin\", \"M√ºnchen\"]`).  \n",
    "3. **Evaluation:** Testen Sie Ihr Few-Shot-Prompting auf 64 zuf√§lligen Beispielen aus `LOC_sentences.txt`. \n",
    "\n",
    "#### Evaluation mit Accuracy\n",
    "Zur Bewertung der Ergebnisse soll die **Accuracy** berechnet werden.  \n",
    "\n",
    "Ein Satz gilt als **korrekt gelabelt**, wenn die vorhergesagte Liste von Ortsnamen exakt mit der Gold-Standard-Liste aus `LOC_sentences.txt` √ºbereinstimmt.\n",
    "\n",
    "#### Beispiel f√ºr das erwartete Verhalten\n",
    "\n",
    "```md\n",
    "Input: \"In Essen haben sich die beiden Br√ºder getroffen.\"\n",
    "Output: [\"Essen\"]\n",
    "```\n",
    "\n",
    "Tipp: Mit `eval()` k√∂nnen Sie die String-Ausgabe des Modells in eine Python-Liste umwandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298a68a",
   "metadata": {},
   "source": [
    "#### Beispiel Prompt\n",
    "\n",
    "```text\n",
    "Erkenne alle Ortsnamen in deutschen S√§tzen und gib sie als Liste zur√ºck.\n",
    "\n",
    "Satz: Die neuerliche Verwaltungsreform aus dem Jahre 1815 brachte f√ºr Passenheim abermals eine neue Kreiszuordnung mit sich .\n",
    "Ortsnamen: ['Passenheim']\n",
    "\n",
    "Satz: Die Hauptbasis wurde 2006 von Alderney nach Jersey verlegt .\n",
    "Ortsnamen: ['Alderney', 'Jersey']\n",
    "\n",
    "Satz: Sie verl√§uft zun√§chst entlang der Bayernstra√üe , √ºberquert diese und erreicht die Haltestelle Dutzendteich , an der Umsteigem√∂glichkeit zur S-Bahn nach Altdorf besteht .\n",
    "Ortsnamen: ['Bayernstra√üe', 'Dutzendteich', 'Altdorf']\n",
    "\n",
    "Satz: Rhein-Kreis Neuss Eigentlich kommt das Thema immer zum Jahresanfang auf den Tisch , wenn die Fraktionen √ºber den Haushaltsplan beraten : Soll sich der Rhein-Kreis Neuss von seinen RWE-Aktien trennen ?\n",
    "Ortsnamen: ['Rhein-Kreis Neuss', 'Rhein-Kreis Neuss']\n",
    "\n",
    "Satz: Eine Goevier ist in der Flugwerft Schlei√üheim ausgestellt .\n",
    "Ortsnamen: ['Schlei√üheim']\n",
    "\n",
    "Satz: Heute ist durch einen roten Ziegelstreifen der Verlauf des an den Turm anschlie√üenden nordwestlichen Mauerrings gekennzeichnet , w√§hrend der s√ºdwestliche Mauerring bis zum Gel√§nde des ehemaligen Franziskanerklosters St . Johannis noch intakt ist .\n",
    "Ortsnamen:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1828f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satz: Heute ist durch einen roten Ziegelstreifen der Verlauf des an den Turm anschlie√üenden nordwestlichen Mauerrings gekennzeichnet , w√§hrend der s√ºdwestliche Mauerring bis zum Gel√§nde des ehemaligen Franziskanerklosters St . Johannis noch intakt ist .\n",
      "Entities: ['St . Johannis']\n"
     ]
    }
   ],
   "source": [
    "# Laden des Datensatzes\n",
    "dataset = []\n",
    "\n",
    "with open(\"LOC_sentences.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        sentence, entities_str = line.split(\"####\")\n",
    "        entities = eval(entities_str)\n",
    "        dataset += [[sentence, entities]]\n",
    "\n",
    "print(\"Satz:\", dataset[0][0])\n",
    "print(\"Entities:\", dataset[0][1])\n",
    "\n",
    "# 5 zuf√§llige Few-Shot Beispiele\n",
    "import random\n",
    "random.seed(42)\n",
    "few_shot_examples = random.sample(dataset, 5)\n",
    "test_data = [item for item in dataset if item not in few_shot_examples][:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0342bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ihr Code steht hier ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fe631",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>L√∂sung anzeigen</b></summary>\n",
    "\n",
    "```python\n",
    "# Prompt erstellen\n",
    "def create_prompt(examples, test_sentence):\n",
    "    prompt = \"Erkenne alle Ortsnamen in deutschen S√§tzen und gib sie als Liste zur√ºck.\\n\\n\"\n",
    "    \n",
    "    for sentence, entities in examples:\n",
    "        prompt += f\"Satz: {sentence}\\nOrtsnamen: {entities}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Satz: {test_sentence}\\nOrtsnamen:\"\n",
    "    return prompt\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(test_data):\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    \n",
    "    for sentence, true_entities in test_data:\n",
    "        prompt = create_prompt(few_shot_examples, sentence)\n",
    "        response = llm_completion(prompt, stop=\"\\n\")\n",
    "        \n",
    "        # Da das LLM manchmal ung√ºltige Python-Ausdr√ºcke zur√ºckgibt, verwenden wir try-except\n",
    "        try:\n",
    "            predicted_entities = eval(response.strip())\n",
    "            if set(predicted_entities) == set(true_entities): # Mit set vergleichen, um Reihenfolge zu ignorieren\n",
    "                correct += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Ausf√ºhrung\n",
    "accuracy = evaluate(test_data) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
