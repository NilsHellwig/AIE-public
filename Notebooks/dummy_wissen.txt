Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction
Nils Constantin Hellwig
Media Informatics Group University of Regensburg Regensburg, Germany nils-constantin.hellwig@ur.de &Jakob Fehle
Media Informatics Group University of Regensburg Regensburg, Germany jakob.fehle@ur.de
\ANDUdo Kruschwitz
Information Science Group University of Regensburg Regensburg, Germany udo.kruschwitz@ur.de &Christian Wolff
Media Informatics Group University of Regensburg Regensburg, Germany christian.wolff@ur.de

Abstract
Aspect sentiment quad prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores almost up to par with those obtained with state-of-the-art fine-tuned models and exceeding previously reported zero- and few-shot performance. In the 20-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 51.54, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were close to fine-tuned models, achieving 68.93 on Rest16 in the 30-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks.


1Introduction
Transformer-based large language models (LLMs) have gained significant attention due to their capability to address a broad spectrum of natural language processing (NLP) tasks, such as text summarization, translation, reading comprehension and text classification (Brown et al., 2020; Dubey et al., 2024). Noteworthy LLMs include Llama-3.1 (Dubey et al., 2024), Gemma-3 (Gemma et al., 2025), and Mixtral (Jiang et al., 2024), which are accessible in various parameter sizes with open model weights and commercial models like GPT-4 (Achiam et al., 2023) and Claude 3 (Anthropic, 2024).

Previous research explored zero- and few-shot scenarios in which the LLM generates outputs with either none or only a few labelled examples provided in the prompt Gou et al. (2023); Zhang et al. (2024). This eliminates the need for supervised model training, such as for small language models1 (SLMs) using annotated datasets (Wang et al., 2023c). This approach is particularly appealing because data annotation is often deemed complex and expensive, both in terms of time or financial cost, thereby complicating the development of text classification solutions tailored to specific tasks (Fehle et al., 2023; Gretz et al., 2023; Li et al., 2023).

An extensively studied task in NLP where manual annotations pose significant challenges is aspect-based sentiment analysis (ABSA) (Zhang et al., 2022). This task facilitates the understanding of customer opinions expressed in reviews or feedback (Pontiki et al., 2014). Unlike traditional sentiment classification, which assigns a single sentiment label (commonly positive, negative, or neutral) to an entire text document, ABSA requires annotators to identify all aspects within the text and determine the sentiment associated with each one (Zhang et al., 2022).

A prominent subtask of ABSA is aspect sentiment quad prediction (ASQP), which provides exceptionally detailed insights into the author’s opinions by identifying four sentiment elements for each opinion: aspect term (a), aspect category (c), sentiment polarity (p) and opinion term (o) (Zhang et al., 2021a). Consequently, the annotation process for training examples is highly demanding, particularly when multiple opinions need to be annotated within a single text.

Previous research has predominantly concentrated on 0- to 10-shot learning, exclusively utilizing the English-language restaurant domain datasets Rest15 and Rest16 introduced by Zhang et al. (2021a).

In this study, we extend the analysis to include up to 50 few-shot examples and evaluate the approach on a diverse series of five datasets. The datasets utilized in this work include Rest15 and Rest16, introduced by Zhang et al. (2021b) and we incorporate the OATS dataset by Chebolu et al. (2024), which consists of hotel reviews from TripAdvisor and online learning reviews collected from Coursera. Finally, we introduce a novel ASQP dataset, comprising annotated sentences from airline reviews, which is published as part of this work.

We considered the following research questions:

RQ1: How does varying the number of few-shot examples (from 0 to 50) impact performance on the ASQP task?

RQ2: How do LLMs perform on the ASQP task compared to SLMs trained on annotated examples?

RQ3: Does self-consistency (SC) prompting (Wang et al., 2022a), where multiple outputs are generated from the same prompt and the most consistent response is selected, improve performance on the ASQP task?

We employed Google’s Gemma-3-27B (Gemma et al., 2025) and report the performance for the smaller-sized Gemma-3-4B. In addition, we report the LLMs’ performance on the target aspect sentiment detection (TASD), which focuses on the identification of (a, c, p)-triplets. All code and results of this study is publicly available on GitHub2.

3Methodology
We utilized LLMs to tackle the ASQP task across 0-, 10-, 20-, 30-, 40-, and 50-shot settings on different datasets. The performance is compared to that achieved using a dedicated training set to fine-tune smaller pre-trained language models. Furthermore, we report performance results for the TASD task.

3.1Evaluation
3.1.1Datasets
Rest15	Rest16	FlightABSA	OATS Coursera	OATS Hotels
# Train	834	1,264	1,351	1,400	1,400
# Test	537	544	387	400	400
# Dev	209	316	192	200	200
# Aspect Categories	13	13	13	28	33
Language	en	en	en	en	en
Domain	restaurant	restaurant	airline	e-learning	hotel
Table 2:Overview of all ASQP datasets considered for evaluation. The datasets cover a range of different numbers of considered aspect categories and domains.
Table 2 presents an overview of the datasets used in this study, including Rest15 and Rest16, along with three additional datasets covering diverse domains.

Rest15 & Rest16: ASQP annotations originate from Zhang et al. (2021a) and the TASD annotations from Wan et al. (2020). This ensured comparability with the performance scores reported in previous research.

FlightABSA: A novel dataset containing 1,930 sentences annotated for ASQP. Properties of the annotated dataset are provided in Appendix B.

OATS Hotels & OATS Coursera: We utilized a subset of two corpora recently introduced by Chebolu et al. (2024) comprising ASQP-annotated sentences from reviews on hotels and e-learning courses. A detailed description of the data preprocessing for the OATS datasets can be found in Appendix A.

For the TASD task, we removed the opinion terms from the quadruples in annotations from FlightABSA, OATS Coursera and OATS Hotels. Subsequently, any duplicate triplets (a, c, p) that appeared twice in a sentence were discarded.

3.1.2Setting
For evaluation, the test dataset was considered for all datasets. An LLM was prompted five times with different seeds (0 to 4) for each combination of ABSA task (ASQP and TASD), dataset and amount of random few-shot examples (0, 10, 20, 30, 40 or 50) taken from the training set in order to get five label predictions. For all seeds, the same few-shot examples were used; however, they were shuffled differently for each prompt execution. The average performance across all five runs is reported.

3.1.3Metrics
As in previous works in the field of ABSA, we report the micro-averaged F1 score as well as precision and recall to assess the model’s performance. The F1 score is the harmonic mean of precision and recall. Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive (Jurafsky and Martin, 2024, p. 67). Recall quantifies the proportion of correctly predicted positive instances out of all actual positive instances in the dataset (Jurafsky and Martin, 2024, p. 67).

Similar to Zhang et al. (2021a), a quad prediction was considered correct if all the predicted sentiment elements are exactly the same as the gold labels. Recognizing the potential interest in class-level performance metrics for subsequent research, we have shared the predicted labels for every evaluated setting in our GitHub repository, allowing detailed class-level analysis.

Refer to caption
Figure 2:The prompt includes both a task description and specification of the output format. The LLM is run with five different seeds and in the case of self-consistency prompting, the tuple that appears most often across the five predictions is incorporated into the final label.
3.2Large Language Models
We employed Gemma-3-27B3 by Google, which comprises 27.4 billion parameters (Gemma et al., 2025). Ollama4 was employed for inference, and the LLMs were loaded with 4-bit quantization. The model was chosen for its efficiency in terms of generated tokens per second, which is a critical factor given the extensive prompt execution requirements. Notably, our study required over 342,720 prompts to be executed, with many few-shot learning prompts encompassing over a thousand tokens. For larger models, such as Llama-3.3-70B Dubey et al. (2024), the required computational costs would have been hardly feasible with our resources. For comparison purposes, we also report performance for the smaller-sized LLM, Gemma-3-4B5.

The experiments were conducted on a NVIDIA RTX A5000 GPU equipped with 24 GB of VRAM. The LLM’s temperature parameter was set to 0.8 and generation was terminated upon encountering the closing square bracket character ("]") signifying the ending of a predicted label.

3.3Prompt
3.3.1Components
We adopted the prompting framework introduced by Gou et al. (2023) with some modifications. The employed prompt is illustrated in Figure 2 and an example is provided in Appendix C. The main components of the prompt include a list of explanation on all considered sentiment elements and the specification of the output format.

Unlike the prompt by Gou et al. (2023), our prompt instructed the LLM to pay attention to case sensitivity when returning aspect and opinion terms. Hence, the identified phrases should appear in the predicted tuple as they do in the sentence, similar to all supervised approaches mentioned in the related work section. Therefore, in the prompt, we clearly stated that the exact phrases should appear in the predicted label.

Since we executed each prompt with five different seeds, we also report the performance when employing the self-consistency prompting technique introduced by Wang et al. (2022a). The key idea is to select the most consistent answer from multiple prompt executions. We adapted the approach for ABSA by incorporating a tuple into the merged label if it appears in the majority of the predicted labels. As illustrated in Figure 2, this corresponds to a tuple appearing in at least 3 out of 5 predicted labels.

3.4Output Validation
Since LLMs such as Gemma-3-27B cannot be strictly constrained to a fixed output format, we programmatically validated the output of the LLM. For the predicted label, several criteria needed to be met for the generation to be considered valid:

• Format: The output must be a list of one or more tuples consisting of strings (quadruples for ASQP, triplets for TASD).
• Sentiment: The sentiment must be either ’positive’, ’negative’ or ’neutral’.
• Aspect category: Only the categories considered for the respective dataset and thus being mentioned in the prompt should be predicted as a part of a tuple.
• Aspect and opinion terms: Both must appear in the given sentence as predicted.
If any of the specified criteria for reasoning or label validation is not met, a regeneration attempt was triggered. If the predicted label was still invalid after 10 attempts, an empty label ([]) was considered as the predicted label.

3.5Baseline Model
We compared the previously mentioned zero- and few-shot conditions against three SOTA baseline approaches, which are, the three best-performing methods for ASQP and TASD on the Rest15 and Rest16 datasets: Paraphrase (Zhang et al., 2021a), DLO (Hu et al., 2022) and MVP (Gou et al., 2023).

Paraphrase (Zhang et al., 2021a):
Paraphrase is used to linearize sentiment quads into a natural language sequence to construct the input target pair.

DLO (Hu et al., 2022):
Dataset-level order is a method designed for ASQP that leverages the order-free property of quadruplets. It identifies and utilizes optimal template orders through entropy minimization and combines multiple effective templates for data augmentation.

MVP (Gou et al., 2023):
Multi-view-Prompting introduces element order prompts. The language model is guided to generate multiple sentiment tuples, with a different element order each, and then selects the most reasonable tuples by a voting mechanism. This method is highly resource-intensive, as multiple input-output pairs are created for each example in the train set, each comprising different sentiment element positions.

For all three approaches, we conducted training using the entire dataset and performed training with only 10, 20, 30, 40, or 50 training examples equally to the ones employed for the few-shot learning conditions. Training was conducted using five different random seeds (0 to 4). Moreover, to facilitate comparisons across datasets, we trained models using 800 training examples, as this represents the largest multiple of 100 examples available for all train sets (900 training examples are not available for Rest15). The results obtained using the full training sets of Rest15 and Rest16 were extracted from the works of Zhang et al. (2021a), Hu et al. (2022), and Gou et al. (2023).

For all methods, we used the hyperparameter configurations used by Zhang et al. (2021a), Hu et al. (2022) and Gou et al. (2023). The only exception was the 10-shot condition, where batch size was set to 8 instead of 16, as the limited number of examples (10) could not form a batch of 16 examples.

Method	 
Prompting
Strategy
# Few-Shot /
# Train
Rest15	Rest16	FlightABSA	 
OATS
Coursera
 	 
OATS
Hotels
 
F1	Pre	Rec	F1	Pre	Rec	F1	Pre	Rec	F1	Pre	Rec	F1	Pre	Rec
Gemma-3-4B	-	0	6.80	7.43	6.26	8.00	8.83	7.31	11.12	13.11	9.66	5.23	5.67	4.86	11.11	14.48	9.02
10	10.95	12.52	9.74	11.25	12.67	10.11	13.02	15.96	11.02	6.67	7.04	6.33	10.53	13.13	8.79
20	16.93	17.94	16.03	18.52	20.06	17.20	11.92	14.00	10.37	9.47	9.59	9.36	14.72	16.02	13.62
30	20.09	20.25	19.95	21.64	23.00	20.43	16.55	18.36	15.08	11.19	11.03	11.35	11.36	11.22	11.54
40	19.40	19.05	19.77	25.42	25.62	25.23	18.34	20.03	16.92	11.26	11.13	11.39	14.01	13.42	14.67
50	24.48	24.62	24.35	24.80	25.46	24.18	22.97	23.93	22.10	11.27	11.61	10.96	16.00	15.87	16.17
SC	0	6.06	28.12	3.40	6.03	28.12	3.38	12.81	45.36	7.46	4.03	25.00	2.19	12.25	52.63	6.93
10	7.86	48.57	4.28	9.63	45.74	5.38	10.53	60.71	5.76	7.42	54.05	3.98	9.80	52.00	5.41
20	18.62	47.67	11.57	20.02	50.00	12.52	8.63	47.46	4.75	10.38	50.88	5.78	14.19	43.88	8.46
30	23.62	51.26	15.35	25.07	54.62	16.27	15.06	46.49	8.98	12.44	46.75	7.17	10.64	26.52	6.66
40	23.44	49.59	15.35	31.13	52.37	22.15	18.23	49.25	11.19	13.62	41.00	8.17	15.12	34.15	9.71
50	30.11	52.34	21.13	27.46	46.97	19.40	26.58	52.50	17.80	14.05	39.09	8.57	16.93	37.26	10.96
Gemma-3-27B	-	0	24.41	22.67	26.44	28.94	27.12	31.01	42.31	39.10	46.10	13.05	11.42	15.22	22.90	22.49	23.33
10	38.19	36.68	39.85	44.35	41.92	47.08	43.04	41.35	44.88	22.07	21.50	22.67	30.47	32.21	28.90
20	36.25	36.99	35.55	49.41	48.53	50.34	42.31	41.72	42.92	24.31	24.56	24.06	36.96	38.61	35.45
30	36.94	37.47	36.43	48.62	48.29	48.96	44.55	44.56	44.54	25.61	26.36	24.90	37.98	40.61	35.67
40	37.19	37.36	37.03	47.82	47.23	48.44	42.52	43.61	41.49	23.30	23.84	22.79	38.38	41.22	35.92
50	39.62	39.65	39.60	47.18	46.52	47.86	44.20	44.05	44.37	23.04	23.26	22.83	39.97	43.41	37.03
SC	0	24.73	23.35	26.29	28.96	27.75	30.29	42.37	39.70	45.42	13.36	11.95	15.14	23.02	22.88	23.16
10	39.95	39.41	40.50	46.23	44.64	47.93	45.24	45.39	45.08	22.31	23.41	21.31	31.41	35.29	28.29
20	36.46	38.70	34.47	51.54	52.83	50.31	43.91	46.17	41.86	26.08	29.28	23.51	39.23	43.84	35.51
30	37.91	41.21	35.09	50.61	51.98	49.31	46.14	48.42	44.07	28.08	33.24	24.30	41.68	48.61	36.48
40	38.54	41.51	35.97	50.03	51.74	48.44	47.16	52.38	42.88	25.86	31.96	21.71	42.12	50.10	36.34
50	41.74	44.57	39.25	51.10	54.55	48.06	48.37	51.95	45.25	25.86	31.96	21.71	43.83	53.39	37.17
MVP
(Gou et al., 2023)
-	10	10.58	12.00	9.46	12.37	14.40	10.84	9.38	11.66	7.84	12.88	14.46	11.62	6.98	8.42	5.97
20	18.71	21.22	16.73	21.49	24.30	19.27	14.27	17.43	12.09	18.85	20.79	17.25	14.30	16.03	12.92
30	24.36	26.54	22.52	27.58	30.83	24.96	22.53	26.82	19.42	21.32	23.25	19.68	20.89	23.17	19.03
40	25.95	27.72	24.40	32.72	33.56	31.94	28.15	32.17	25.03	20.21	22.02	18.68	24.71	27.00	22.78
50	30.20	31.07	29.38	33.32	34.75	32.02	33.12	35.09	31.38	22.07	24.16	20.32	29.91	33.08	27.31
800	50.02	48.99	51.09	58.09	56.31	59.97	57.46	56.23	58.74	30.26	29.91	30.62	53.37	52.41	54.36
Full	51.04	-	-	60.39	-	-	57.90	56.09	59.83	32.50	32.04	32.97	55.03	54.38	55.69
DLO
(Hu et al., 2022)
10	4.37	4.64	4.13	5.18	5.49	4.91	4.87	6.15	4.03	4.47	5.03	4.02	3.53	3.68	3.41
20	12.06	14.37	10.39	13.84	14.42	13.32	9.75	12.09	8.17	10.79	11.88	9.88	8.16	6.86	10.10
30	18.71	18.16	19.32	24.06	24.71	23.45	16.63	18.13	15.39	17.05	17.73	16.41	17.71	17.54	17.89
40	22.87	21.36	24.60	26.92	25.94	27.98	23.75	26.24	21.69	17.22	18.38	16.22	22.65	23.01	22.33
50	26.63	24.92	28.60	29.57	29.09	30.06	28.74	28.30	29.22	19.08	20.44	17.89	27.20	28.54	25.99
800	49.87	48.59	51.22	59.44	57.73	61.25	57.42	56.03	58.88	30.83	30.37	31.31	54.40	53.39	55.45
Full	48.18	47.08	49.33	59.79	57.92	61.80	58.33	56.67	60.10	32.54	32.03	33.07	55.45	54.39	56.56
Paraphrase
(Zhang et al., 2021a)
10	1.32	1.64	1.11	3.56	4.02	3.23	3.44	4.34	2.85	4.75	5.35	4.26	2.63	3.66	2.06
20	5.48	6.78	4.60	11.14	10.54	11.91	3.48	4.39	2.88	9.51	10.64	8.61	5.34	6.36	4.65
30	9.47	9.54	9.46	7.18	8.44	6.28	3.60	4.55	2.98	11.39	12.84	10.24	5.13	6.48	4.26
40	17.61	17.07	18.19	20.15	20.69	19.67	13.81	15.09	12.78	16.43	17.79	15.26	14.96	15.99	14.08
50	25.55	24.58	26.62	23.50	23.75	23.25	17.98	18.58	17.42	19.38	20.72	18.21	23.09	23.67	22.59
800	46.32	45.61	47.07	56.88	55.65	58.17	54.96	54.10	55.86	30.79	30.63	30.96	53.65	52.57	54.77
Full	46.93	46.16	47.72	57.93	56.63	59.30	57.76	57.37	58.17	32.34	32.06	32.63	53.87	52.61	55.19
Table 3:Performance scores for ASQP. For the Rest15 and Rest16 datasets, performance scores achieved when employing the full training set ("Full") are taken from Gou et al. (2023), Hu et al. (2022) and Zhang et al. (2021b) for MVP, DLO and Paraphrase, respectively. The best score achieved by a method is presented in bold.
4Results
The performance scores for the evaluated configurations are shown in Table 3 for the ASQP task and in Appendix D for the TASD task. Detailed performance scores focusing on individual sentiment elements are provided in Appendix E. Notably, for both tasks, we performed t-tests with Bonferroni correction (padj < .05) to examine whether significant differences exist between the F1 scores of the evaluated conditions (corresponding to the number of rows in Figure 3). No significant differences were observed.

Performance gains with an increasing number of few-shot examples. In most cases, increasing the number of few-shot examples resulted in incremental improvements in F1 scores across both ASQP and TASD tasks. The difference between zero- and few-shot prompting is substantial. For instance, on the Rest16 dataset under the SC prompting condition, the F1 score improved from 28.96 (0-shot) to 51.10 (50-shot) for the ASQP task. To further highlight this trend, we provide line plots (see Figure 4) that depict the influence of the number of few-shot examples on the F1 scores across all tasks, datasets, and models.

LLM performance slightly lower compared to SOTA fine-tuned approaches. For both TASD and ASQP, the performance achieved through zero- and few-shot prompting did not surpass that obtained when the entire training set was utilized. For example, on the Rest16 dataset, Gemma-3-27B achieved 68.93, which is slightly below the best F1 score achieved by a fine-tuned approach (MVP: 72.76). However, the best F1 scores achieved by Gemma-3-27B in the TASD task were often close to those achieved by fine-tuned approaches employing 800 or all examples from the training set. In case only 10 to 50 annotated examples were used for prompting or training, few-shot prompting consistently outperformed fine-tuning approaches across all sample sizes, with only a few exceptions.

Massive performance enhancements achieved through self-consistency. SC enabled considerable boosts of the F1 score, regardless of the amount of few-shot examples. However, recall was occasionally higher without SC. Precision, on the other hand, was improved with SC in both tasks and across datasets. For instance, in the case of Gemma-3-4B, precision was increased in most instances.

The LLM’s parameter size matters. Gemma-3-4B demonstrated lower performance in terms of F1 scores for both ASQP and TASD. Across the five datasets, the F1 scores in the ASQP task were approximately 10 percentage points lower when using Gemma-3-4B instead of Gemma-3-27B. For example, on the Rest15 dataset, the best F1 score achieved with Gemma-3-4B was 44.20, while the best score for Gemma-3-27B was 62.12 on the TASD task.

Lower performance in identifying opinion terms compared to other sentiment elements. As shown in the tables in Appendix E, performance in identifying sentiment (positive, negative, or neutral) is highly performant, with F1 scores exceeding 90. However, performance in identifying aspect and opinion terms is comparatively much lower.

5Discussion
The results demonstrated performance improvements in F1 scores for both ASQP and TASD as the number of few-shot examples increases, highlighting the gap between zero- and few-shot prompting. In this chapter, we put the results of this work into the context of previous research and provide an outlook on the direction of future work.

New SOTA performance of LLMs. The LLM zero- and few-shot learning performance scores reported in previous studies by Gou et al. (2023), Zhang et al. (2024) and Bai et al. (2024) for the ASQP task on the Rest15 and Rest16 datasets fall below those achieved by Gemma-3-27B in both zero- and 10-shot learning settings. The only exception is Rest15, where ChatABSA (Bai et al., 2024) outperformed Gemma-3-27B in zero-shot learning except for TASD + Rest16. Unlike prior studies, which have primarily evaluated up to 10-shot settings, we extended the investigation to a 10- to 50-shot setting for the first time. In this expanded range, Gemma-3-27B achieved notable F1 scores exceeding 50 for the ASQP task (e.g., Rest16 with SC: 51.54) and surpassing 60 for the TASD task (e.g., Rest16 with SC: 68.93). Notably, these substantial gains are also attributed to the use of SC prompting. Furthermore, this is in contrast to the work of Wu et al. (2024), whose SC approach for E2E-ABSA did not lead to an improvement in performance.

Model size and prompting strategy affect few-shot performance. Although Gemma-3-27B achieved competitive results in both ASQP and TASD, its performance remained slightly below fine-tuned SOTA approaches such as those by Gou et al. (2023), Zhang et al. (2021b), and Hu et al. (2022) when full training sets were employed. However, in scenarios with limited annotated examples, few-shot prompting consistently outperformed fine-tuning. The parameter size of the model also influenced performance, with Gemma-3-27B consistently outperforming its smaller counterpart, Gemma-3-4B.

Directions for enhancing low-resource task performance. Building on the promising results of this study, future research could focus on improving low-resource task performance through advanced prompt engineering techniques. Approaches such as chain-of-thought prompting (Wei et al., 2022) or plan-and-solve prompting (Wang et al., 2023a), which allowed for performance gains in other NLP tasks, hold significant potential. Furthermore, refining annotation guidelines or representing labels as natural language text, as proposed by Zhang et al. (2021b), could contribute to improved outcomes. Bigger LLMs, e.g. with 70B parameters, may provide additional performance benefits, given that our 27B model demonstrated superior results compared to the 4B variant.

Exploring less complex tasks and many-shot learning. In a broader context, future research could extend our approach to less complex tasks, in terms of the amount of considered sentiment elements, such as E2E-ABSA or aspect category sentiment analysis (ACSA) which focuses on aspect category and the sentiment expressed towards them. Beyond the low-resource setting considered in this study, one could explore the so-called "many-shot in-context learning" paradigm described by Agarwal et al. for ABSA, where hundreds or even the full training set is provided in the prompt. Observing that our approach achieved performance scores on the TASD task close to fine-tuned models, future work could investigate whether further increasing the number of shots lead to surpassing fine-tuned approaches.

Limitations
This study evaluated the performance of LLMs on ASQP tasks across a broad selection of datasets, few-shot settings, and LLM configurations. However, a limitation of this work is the selection of employed LLMs. We only employed LLMs comprising 4 or 27 billion parameters. Bigger-sized models such as Llama-3-70B (Dubey et al., 2024) or commercial models were not considered due to their prohibitive computational and financial costs. In order to evaluate each setting for a considered LLM, we executed a total of 171,360 prompts. Due to the amount of tokens in each prompt, the associated cost implications are substantial: about 125 hours (5 days) for Gemma-3-27B and 59 hours (2 days) for Gemma-3-4B. Hence, the time would further increase with an even bigger LLM in terms of parameter size. For commercial models such as GPT-4, executing all prompts would result in massive costs.

Finally, we must highlight the issue of potential data contamination, as it is the case for the previous studies introduced in the related work section. Meaning, it cannot be ruled out that the publicly available annotated datasets used in this study (except for FlightABSA) were included in the training data for both Gemma-3-4B and Gemma-3-27B.

Ethics Statement
All results and code used in this study are publicly available. The dataset we introduced, FlightABSA, is available upon request. We want to prevent the annotated dataset from being available online and then being inadvertently collected for pre-training LLMs.
